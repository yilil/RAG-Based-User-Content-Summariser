import logging
import re
import time
import json
from concurrent.futures import ThreadPoolExecutor
from django.shortcuts import render
from django.http import JsonResponse
from django.views.decorators.http import require_POST
from search_process.langchain_parser import parse_langchain_response
from search_process.prompt_generator import generate_prompt
from search_process.prompt_sender import send_prompt
from search_process.query_classification.classification import classify_query
from django_apps.memory.service import MemoryService
from django_apps.search.models import RedditContent, StackOverflowContent, RednoteContent, ContentIndex
from django_apps.search.index_service.base import IndexService
from django_apps.search.index_service.hybrid_retriever import HybridRetriever
from typing import List, Dict
from langchain.docstore.document import Document
from urllib.parse import quote
from django_apps.search.crawler_config import REDNOTE_LOGIN_COOKIES

# Initialize logger
logger = logging.getLogger(__name__)

# Initialize ThreadPoolExecutor
executor = ThreadPoolExecutor(max_workers=5)

# Initialize the shared index_service
index_service = IndexService(platform="reddit")  # Declare a global variable to hold the shared instance

def search(request):
    """
    å¤„ç†æœç´¢è¯·æ±‚ï¼š
    1. ä½¿ç”¨FAISSè¿›è¡Œç›¸ä¼¼æ–‡æ¡£æ£€ç´¢
    2. å¦‚æœæ²¡æœ‰ç›¸å…³ç»“æœ & å¼€å¯äº†å®æ—¶æŠ“å–ï¼Œåˆ™è°ƒç”¨å®æ—¶æŠ“å–
    3. è°ƒç”¨Geminiå¤„ç†æœç´¢ç»“æœ
    """
    print("Request Method:", request.method)
    print("Request Headers:", request.headers)
    print("Request Body:", request.body)
    global index_service

    logger.info("Received search request")
    logger.debug(f"Request method: {request.method}")
    logger.debug(f"POST data: {request.POST}")

    # é»˜è®¤è¿”å›å˜é‡
    answer = ""
    metadata = {}
    retrieved_docs = []
    llm_model = ""

    # å¤„ç† POST è¯·æ±‚
    data = json.loads(request.body.decode('utf-8'))
    search_query = data.get('search_query')
    platform = data.get('source')
    filter_value = data.get('filter_value', None)
    llm_model = data.get('llm_model', llm_model)
    session_id = data.get('session_id')
    topic = data.get('topic')
    
    # è·å–å®æ—¶æŠ“å–è®¾ç½®
    real_time_crawling_enabled = data.get('real_time_crawling_enabled', False)

    if not session_id:
        request.session.create()  # åˆ›å»ºæ–°ä¼šè¯
        session_id = request.session.session_key
    recent_memory = MemoryService.get_recent_memory(session_id, limit=10, platform=platform, topic=topic)

    if not search_query:
        return JsonResponse({
                'result': answer,
                'metadata': metadata,
                'llm_model': llm_model,
                'history': recent_memory
            },
            json_dumps_params={'ensure_ascii': False}
        )

    logger.info(f"Processing search query: {search_query} with model: {llm_model} and option: {filter_value}")

    try:
        #1. FAISSæœç´¢
        if not platform:
            platform = 'reddit'
        # æ›´æ–°å¹³å°å¹¶ç¡®ä¿åŠ è½½äº†å¯¹åº”çš„ç´¢å¼•
        index_service.platform = platform
        index_service.faiss_manager.set_platform(platform)

        index_count = index_service.faiss_manager.get_index_size()
        logger.info(f"å½“å‰{platform}å¹³å°ç´¢å¼•åŒ…å«{index_count}æ¡è®°å½•")

        # åˆå§‹åŒ– HybridRetriever
        hybrid_retriever = HybridRetriever(
            faiss_manager=index_service.faiss_manager,
            embedding_model=index_service.embedding_model,
            bm25_weight=0.55,
            embedding_weight=0.35,
            vote_weight=0.1,
            l2_decay_beta=6.0
        )

        # è·å–æœ€ç»ˆçš„ top_k retrieved_documents
        print(f"--- [views.search] è°ƒç”¨ hybrid_retriever.retrieve (Query: '{search_query}')... ---")
        retrieved_docs = hybrid_retriever.retrieve(query=search_query, top_k=5, relevance_threshold=0.6) # å¯ä»¥åŠ¨æ€è°ƒæ•´
        print(f"--- [views.search] hybrid_retriever.retrieve è¿”å›äº† {len(retrieved_docs)} ä¸ªæ–‡æ¡£ ---")

        # --- å…³é”®æ‰“å°ï¼šæ£€æŸ¥ä¼ é€’ç»™ generate_prompt çš„æ–‡æ¡£å…ƒæ•°æ® ---
        print(f"--- [views.search] å‡†å¤‡è°ƒç”¨ generate_prompt, æ£€æŸ¥ retrieved_docs å…ƒæ•°æ® (Top 5): ---")
        for i, doc in enumerate(retrieved_docs[:5]):
            if hasattr(doc, 'metadata'):
                print(f"  Doc {i+1}: Metadata = {doc.metadata}")
            else:
                print(f"  Doc {i+1}: Error - Document has no metadata attribute.")
        # --- ç»“æŸå…³é”®æ‰“å° ---

        # 2. ç”Ÿæˆprompt
        classification = re.search(r">(\d+)<", classify_query(search_query, llm_model)).group(1)

        # æ£€æŸ¥æ˜¯å¦æœ‰æœç´¢ç»“æœï¼Œå¦‚æœæ²¡æœ‰ä¸”å¼€å¯äº†å®æ—¶æŠ“å–ï¼Œåˆ™è°ƒç”¨å®æ—¶æŠ“å–
        if not retrieved_docs or len(retrieved_docs) < 1: # æ”¹top_kçš„æ—¶å€™æ³¨æ„è¿™é‡Œ
            print(f"æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç»“æœ: {search_query}")
            logger.info(f"æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç»“æœ: {search_query}")
            
            # å¦‚æœå¼€å¯äº†å®æ—¶æŠ“å–åŠŸèƒ½ï¼Œè°ƒç”¨å®æ—¶æŠ“å–
            if real_time_crawling_enabled:
                logger.info(f"å®æ—¶æŠ“å–å·²å¯ç”¨ï¼Œå¼€å§‹ä¸ºæŸ¥è¯¢æŠ“å–: {search_query}")
                return handle_real_time_crawling(
                    search_query, 
                    platform, 
                    session_id, 
                    llm_model, 
                    recent_memory,
                    classification
                )
            
            # å¦‚æœæ²¡æœ‰å¼€å¯å®æ—¶æŠ“å–ï¼Œè¿”å›æ— ç»“æœæç¤º
            answer = f"æŠ±æ­‰ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°å…³äº'{search_query}'çš„ç›¸å…³ä¿¡æ¯ã€‚è¯•è¯•å¼€å¯å®æ—¶æœç´¢è·å–æœ€æ–°ç»“æœã€‚"
            metadata = {'no_results': True}
            
            return JsonResponse({
                'result': answer,
                'metadata': metadata,
                'llm_model': llm_model,
                'history': recent_memory
            },
            json_dumps_params={'ensure_ascii': False}
        )


        # *** -> å¦‚æœæ˜¯æ¨èç±»æŸ¥è¯¢ï¼Œç›´æ¥ä½¿ç”¨process_recommendationså¤„ç† ***
        if classification == '1':  # æ¨èç±»æŸ¥è¯¢
            logger.info("ä½¿ç”¨æ¨èç±»å¤„ç†é€»è¾‘å¤„ç†æŸ¥è¯¢")
            
            # ä½¿ç”¨ResultProcessorå¤„ç†æ¨è -> è¿™é‡Œé¡µé¢çš„æ˜¾ç¤ºä¸Šè¿˜æœ‰é—®é¢˜ & è²Œä¼¼åªæœ‰è·‘mockæ•°æ®ï¼Œä½†æ˜¯retrieveåˆ°äº†æ–‡æ¡£
            top_for_prompt = sorted(
                retrieved_docs,
                key=lambda d: d.metadata.get('relevance_score', 0),
                reverse=True
            )[:5]
            processed_results = index_service.result_processor.process_recommendations(
                documents=top_for_prompt,
                query=search_query,
                top_k=5  # å¯é…ç½®çš„æ¨èæ•°é‡
            )
            
            # æ ¼å¼åŒ–æ¨èç»“æœ
            answer = format_recommendation_results(processed_results)
            metadata = {'query_type': 'recommendation', 'processing': 'direct'}
            
            # å°†å¯¹è¯æ·»åŠ åˆ°è®°å¿†
            MemoryService.add_to_memory(
                session_id,
                search_query,
                answer
            )
            
            # ç›´æ¥è¿”å›ç»“æœï¼Œä¸ç»è¿‡LLMå¤„ç†
            return JsonResponse({
                'result': answer,
                'metadata': metadata,
                'llm_model': "recommendation_processor",  # æ ‡è®°ä½¿ç”¨äº†æ¨èå¤„ç†å™¨
                'history': MemoryService.get_recent_memory(session_id)
            },
            json_dumps_params={'ensure_ascii': False}
        )


        # *** -> å¦‚æœæ˜¯éæ¨èç±»æŸ¥è¯¢ï¼Œèµ°æ­£å¸¸å¤„ç†é€»è¾‘ ***
        prompt = generate_prompt(search_query, retrieved_docs, recent_memory, platform, classification)

        future = executor.submit(
            send_prompt, 
            prompt, 
            llm_model
        )
        response = future.result()

        answer, metadata = parse_langchain_response(response)
        MemoryService.add_to_memory(session_id, search_query, answer)

    except Exception as e:
        logger.error(f"Error in search process: {str(e)}", exc_info=True)
        answer = "An unexpected error occurred. Please try again later."
        metadata = {}

    print(answer)
    recent_memory = MemoryService.get_recent_memory(session_id)
    return JsonResponse({
            'result': answer,
            'metadata': metadata,
            'llm_model': llm_model,
            'history': recent_memory
        },
            json_dumps_params={'ensure_ascii': False}
        )



# Initialization of indexing and embeddings
@require_POST
def index_content(request):
    """
    åˆå§‹åŒ–å†…å®¹ç´¢å¼•ï¼ˆå¢é‡ç´¢å¼•ç‰ˆæœ¬ï¼‰ï¼š
      1. å¯¹æ¯ä¸ªå¹³å°ï¼Œå…ˆä»ç£ç›˜åŠ è½½å·²æœ‰ç´¢å¼•
      2. æ‰¾å‡ºæ•°æ®åº“ä¸­æ–°çš„è®°å½•ï¼ˆä¸åœ¨ContentIndexçš„ï¼‰
      3. ä»…å¯¹æ–°è®°å½•åšembeddingå¹¶å†™å…¥å†…å­˜ç´¢å¼•
      4. ä¿å­˜åˆå¹¶åçš„ç´¢å¼•åˆ°æœ¬åœ°ç£ç›˜
    """

    global index_service

    results = {}

    start_time = time.time()
    logger.info("Starting content indexing process (incremental).")

    source_filter = request.POST.get('source')
    if not source_filter:
        platforms = ['reddit', 'stackoverflow', 'rednote']
    else:
        platforms = [source_filter]

    try:
        for platform in platforms:
            logger.info(f"Processing platform: {platform}")
            if platform not in ['reddit', 'stackoverflow', 'rednote']:
                logger.warning(f"Unknown platform: {platform}, skip.")
                continue

            # æ›´æ–°å¹³å°
            index_service.platform = platform
            index_service.faiss_manager.set_platform(platform)
            
            # å°è¯•åŠ è½½å·²æœ‰ç´¢å¼•
            try:
                loaded = index_service.faiss_manager.load_index()
                if not loaded:
                    logger.info(f"æ²¡æœ‰æ‰¾åˆ°ç°æœ‰ç´¢å¼•ï¼Œä¸º {platform} åˆ›å»ºæ–°ç´¢å¼•")
                    index_service.faiss_manager.create_empty_index()
                    # ç¡®ä¿ä¿å­˜ç©ºç´¢å¼•
                    index_service.faiss_manager.save_index()
                    logger.info(f"ç©ºç´¢å¼•å·²åˆ›å»ºå¹¶ä¿å­˜åˆ°ç£ç›˜ï¼š{platform}")
            except Exception as e:
                logger.error(f"åŠ è½½æˆ–åˆ›å»ºç´¢å¼•å¤±è´¥: {str(e)}")
                results[platform] = f"error: {str(e)}"
                continue

            # æ ¹æ®å¹³å°è·å–"æœªç´¢å¼•çš„"æ–°å†…å®¹
            if platform == 'reddit':
                model_cls = RedditContent
            elif platform == 'stackoverflow':
                model_cls = StackOverflowContent
            else:  # 'rednote'
                model_cls = RednoteContent

            # *** ç›®å‰æ”¹æˆç”¨threadidæ¥åˆ¤æ–­æ˜¯å¦é‡å¤
            unindexed = model_cls.objects.exclude(
            thread_id__in=ContentIndex.objects.filter(source=platform).values('thread_id')
            )   

            count_unindexed = unindexed.count()

            if count_unindexed > 0:
                logger.info(f"Found {count_unindexed} new {platform} items to index.")
                # è°ƒç”¨ index_platform_content() å†…éƒ¨ä¼šï¼š
                # 1) ä»…å¯¹ unindexed æ•°æ®åš embedding + add_texts
                # 2) å°†å…¶å†™å…¥ ContentIndex
                # 3) è°ƒç”¨ save_index() å†å†™å›ç£ç›˜
                index_service.indexer.index_platform_content(platform=platform, unindexed_queryset=unindexed)
                # æ·»åŠ å†…å®¹åä¿å­˜ç´¢å¼•
                # index_service.faiss_manager.save_index()
                logger.info(f"Saved FAISS index for {platform}")
            else:
                logger.info(f"No new {platform} items to index.")

        duration = time.time() - start_time
        logger.info(f"Indexing completed in {duration:.2f} seconds.")
        return JsonResponse({
            "status": "success",
            "message": "Incremental indexing completed successfully",
            "duration": round(duration, 2)
        })

    except Exception as e:
        logger.error(f"Indexing failed: {str(e)}")
        return JsonResponse({
            "status": "error",
            "message": str(e)
        }, status=500)
        
def sessionKey(request):
    session_id = request.session.session_key
    if not session_id:
        request.session.create()  # åˆ›å»ºæ–°ä¼šè¯
        session_id = request.session.session_key
    data = json.loads(request.body.decode('utf-8'))
    platform = data.get('platform')
    topic = data.get('topic')
    MemoryService.get_or_create_memory(session_id, platform, topic)
    return JsonResponse({
        'session_id': session_id
    })

def getMemory(request):
    session_id = request.GET.get('session_id')
    if not session_id:
        return JsonResponse({
            'error': 'session_id is required'
        }, status=400)
    memory = MemoryService.get_recent_memory(session_id)
    return JsonResponse({
        'memory': memory
    })

def getAllChat(request):
    sessions = MemoryService.get_all_sessions()
    sessions_data = [
        {
            "session_id": s.session_id,
            "platform": s.platform,
            "topic": s.topic,
            "memory_data": s.memory_data,
            "updated_at": s.updated_at.isoformat()  # DateTime éœ€è¦è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        }
        for s in sessions
    ]
    return JsonResponse({
        'sessions': sessions_data
    })


def format_recommendation_results(results: List[Document]) -> str:
    if not results:
        return "æœªæ‰¾åˆ°ç›¸å…³æ¨èã€‚"

    lines = ["# æ ¹æ®æ‚¨çš„æŸ¥è¯¢ï¼Œä¸ºæ‚¨æ¨èä»¥ä¸‹é€‰é¡¹ï¼š\n"]
    for i, doc in enumerate(results, 1):
        m = doc.metadata
        # Use .get for safety, providing default values
        name = m.get('name', 'N/A')
        avg_rating = m.get('avg_rating', 0.0)
        total_upvotes = m.get('total_upvotes', 0)
        mentions = m.get('mentions', 0)
        # Ensure sentiment_counts exists and has the expected structure, default if missing
        sentiment_counts = m.get('sentiment_counts', {'positive': 0, 'neutral': 0, 'negative': 0})
        summary = m.get('summary', 'No summary available.')
        posts = m.get('posts', []) # Default to empty list if 'posts' key is missing

        # Header line using sentiment_counts safely
        lines.append(
            f"{i}. **{name}**  ğŸŒŸ{avg_rating:.1f}  ğŸ‘{total_upvotes}  ğŸ“{mentions}"
            f"   Sentiment: +{sentiment_counts.get('positive', 0)}, ~{sentiment_counts.get('neutral', 0)}, -{sentiment_counts.get('negative', 0)}"
        )

        # Sentiment breakdown line using sentiment_counts safely
        lines.append(
            f"â–¶ï¸ Sentiment Breakdown: Positive {sentiment_counts.get('positive', 0)} | Neutral {sentiment_counts.get('neutral', 0)} | Negative {sentiment_counts.get('negative', 0)}"
        )

        # General Summary
        lines.append("General Summary:")
        try:
            # Split summary into sentences (adjust regex if needed)
            bullets = re.split(r'(?<=[.?!])\s+', summary)
        except Exception:
             bullets = [summary] # Fallback if regex fails

        for sentence in bullets:
            # Ensure sentence is not None or empty before stripping/appending
            if sentence and sentence.strip():
                lines.append(f"- {sentence.strip()}")

        # Detailed Reviews
        lines.append("\nDetailed Reviews:")
        # Iterate safely over posts (which defaults to [] if missing)
        for post in posts[:10]:
             # Use .get for safety within the post dictionary
             content = post.get('content', 'N/A')
             upvotes = post.get('upvotes', 0)
             # No 'sentiment' variable is used here in the correct version
             lines.append(f"- {content}  (ğŸ‘{upvotes})")

        lines.append("")  # ç©ºè¡Œ

    # Comparison Table
    lines.append("## Comparison Table\n")
    lines.append("| Name | ğŸŒŸRating | ğŸ‘Upvotes | ğŸ“Mentions |")
    lines.append("|------|----------|----------|-----------|")
    for doc in results:
        m = doc.metadata
        # Use .get for safety here too
        name = m.get('name', 'N/A')
        avg_rating = m.get('avg_rating', 0.0)
        total_upvotes = m.get('total_upvotes', 0)
        mentions = m.get('mentions', 0)
        # Ensure score and rank are handled safely if needed for the table later
        # score = m.get('score', 0.0)
        # rank = m.get('rank', 99)
        lines.append(f"| {name} | {avg_rating:.1f} | {total_upvotes} | {mentions} |")

    return "\n".join(lines)

def handle_real_time_crawling(search_query, platform, session_id, llm_model, recent_memory, classification=None):
    """
    å¤„ç†å®æ—¶æŠ“å–åŠŸèƒ½çš„è¾…åŠ©å‡½æ•°ã€‚
    å½“æ•°æ®åº“æœç´¢æ— ç»“æœä¸”å¼€å¯äº†å®æ—¶æŠ“å–æ—¶è°ƒç”¨ã€‚
    """
    logger.info(f"å¼€å§‹å®æ—¶æŠ“å–: {search_query}, å¹³å°: {platform}")
    
    try:
        # å¦‚æœåˆ†ç±»æœªæä¾›ï¼Œè¿›è¡Œåˆ†ç±»
        if classification is None:
            classification = re.search(r">(\d+)<", classify_query(search_query, llm_model)).group(1)
            logger.info(f"æŸ¥è¯¢'{search_query}'è¢«åˆ†ç±»ä¸º: {classification}")
        
        # æ ¹æ®å¹³å°é€‰æ‹©é€‚å½“çš„çˆ¬è™«
        if platform == 'reddit':
            try:
                from django_apps.search.reddit_crawler import create_reddit_instance, fetch_and_store_reddit_posts
                
                # åˆ›å»ºRedditå®ä¾‹
                reddit = create_reddit_instance()
                
                # ä¼˜åŒ–æŸ¥è¯¢è¯
                query_words = search_query.split()
                if len(query_words) > 3:
                    # ç§»é™¤å¸¸è§åœç”¨è¯
                    stopwords = {'a', 'an', 'the', 'is', 'are', 'was', 'were', 'what', 'which', 'how', 'why', 'when', 'who'}
                    optimized_query = ' '.join([word for word in query_words if word.lower() not in stopwords])
                else:
                    optimized_query = search_query
                    
                # å¯¹äºæ¨èç±»æŸ¥è¯¢æ·»åŠ æœç´¢ä¿®é¥°ç¬¦
                if 'recommend' in search_query.lower() or 'suggest' in search_query.lower() or classification == '1':
                    optimized_query += ' recommend OR suggest OR best'
                
                logger.info(f"ä¼˜åŒ–åçš„RedditæŸ¥è¯¢: {optimized_query}")
                
                # æŠ“å–å¸–å­
                crawled_posts = fetch_and_store_reddit_posts(reddit, optimized_query, limit=5)
            except Exception as e:
                logger.error(f"Redditçˆ¬è™«å¼‚å¸¸: {str(e)}", exc_info=True)
                return JsonResponse({
                    'result': f"æŠ±æ­‰ï¼Œåœ¨æŠ“å–Redditä¿¡æ¯æ—¶é‡åˆ°é—®é¢˜: {str(e)}",
                    'metadata': {'crawler_error': str(e)},
                    'llm_model': llm_model,
                    'history': recent_memory
                })
            
        elif platform == 'stackoverflow':
            try:
                from django_apps.search.stackoverflow_crawler import fetch_and_store_stackoverflow_data
                
                # å¯¹äºStackOverflowä¸éœ€è¦å¤ªå¤šä¼˜åŒ–
                crawled_posts = fetch_and_store_stackoverflow_data(search_query, limit=5)
            except Exception as e:
                logger.error(f"StackOverflowçˆ¬è™«å¼‚å¸¸: {str(e)}", exc_info=True)
                return JsonResponse({
                    'result': f"æŠ±æ­‰ï¼Œåœ¨æŠ“å–StackOverflowä¿¡æ¯æ—¶é‡åˆ°é—®é¢˜: {str(e)}",
                    'metadata': {'crawler_error': str(e)},
                    'llm_model': llm_model,
                    'history': recent_memory
                })
            
        # --- Update RedNote real-time crawling ---
        elif platform == 'rednote':
            try:
                from django_apps.search.crawler import crawl_rednote_page

                target_url = generate_xhs_search_url(search_query)
                logger.info(f"Generated RedNote search URL: {target_url}")

                # ä½¿ç”¨ä» crawler_config å¯¼å…¥çš„ Cookies
                cookies_to_use = REDNOTE_LOGIN_COOKIES
                if not cookies_to_use:
                     raise ValueError("RedNote cookies not configured or empty in crawler_config.py.")

                crawled_posts = crawl_rednote_page(url=target_url, cookies=cookies_to_use, immediate_indexing=False)
                logger.info(f"RedNote crawler attempted search URL, found {len(crawled_posts)} potential posts.")

            except ImportError:
                 logger.error(f"RedNote crawler function not found.")
                 return JsonResponse({
                    'result': f"æŠ±æ­‰ï¼ŒRedNoteå®æ—¶æŠ“å–åŠŸèƒ½é…ç½®é”™è¯¯ã€‚",
                    'metadata': {'crawler_error': 'ImportError', 'crawl_attempted': True, 'platform': platform},
                    'llm_model': llm_model,
                    'history': recent_memory
                })
            except Exception as e:
                logger.error(f"RedNoteçˆ¬è™«å¼‚å¸¸ (URL: {target_url}): {str(e)}", exc_info=True) # Log URL
                return JsonResponse({
                    'result': f"æŠ±æ­‰ï¼Œåœ¨æŠ“å–RedNoteä¿¡æ¯æ—¶é‡åˆ°é—®é¢˜: {str(e)}",
                    'metadata': {'crawler_error': str(e), 'crawl_attempted': True, 'platform': platform},
                    'llm_model': llm_model,
                    'history': recent_memory
                })
        # --- End RedNote update ---

        else:
            return JsonResponse({
                'result': f"æŠ±æ­‰ï¼Œå¹³å°'{platform}'ä¸æ”¯æŒå®æ—¶æŠ“å–åŠŸèƒ½ã€‚",
                'metadata': {'error': 'unsupported_platform'},
                'llm_model': llm_model,
                'history': recent_memory
            })
        
        if not crawled_posts:
            logger.warning(f"æœªæ‰¾åˆ°æŸ¥è¯¢ç»“æœ: {search_query}, å¹³å°: {platform}")
            return JsonResponse({
                'result': f"æŠ±æ­‰ï¼Œæˆ‘æ— æ³•åœ¨{platform}æ‰¾åˆ°å…³äº'{search_query}'çš„ç›¸å…³ä¿¡æ¯ã€‚",
                'metadata': {'no_results': True, 'crawl_attempted': True},
                'llm_model': llm_model,
                'history': recent_memory
            })
        
        logger.info(f"æˆåŠŸæŠ“å– {len(crawled_posts)} æ¡å†…å®¹ï¼ŒæŸ¥è¯¢: {search_query}, å¹³å°: {platform}")
        
        # å¦‚æœæ˜¯æ¨èç±»æŸ¥è¯¢(1)ï¼Œåˆ™ä½¿ç”¨ç›´æ¥å¤„ç†æ–¹å¼
        if classification == '1':
            logger.info("ä½¿ç”¨æ¨èç±»å¤„ç†é€»è¾‘å¤„ç†å®æ—¶æŠ“å–ç»“æœ")
            
            # åˆ›å»ºæ ‡å‡†æ ¼å¼çš„æ–‡æ¡£å¯¹è±¡
            mock_retrieved_docs = []
            for post in crawled_posts:
                # åˆ›å»ºç¬¦åˆDocumentæ ¼å¼çš„å¯¹è±¡
                mock_doc = Document(
                    page_content=post.content,
                    metadata={
                        "source": platform,
                        "score": post.upvotes if hasattr(post, 'upvotes') else 0,
                        "url": post.url,
                        "title": post.thread_title,
                        "thread_id": post.thread_id,
                        "author": post.author_name,
                        "upvotes": post.upvotes if hasattr(post, 'upvotes') else 0
                    }
                )
                mock_retrieved_docs.append(mock_doc)
            
            # ä½¿ç”¨ResultProcessorå¤„ç†æ¨è
            try:
                # å°è¯•ä½¿ç”¨ResultProcessorï¼Œä½†å‡†å¤‡å¥½ç›´æ¥å¤‡ç”¨æ–¹æ¡ˆ
                if len(mock_retrieved_docs) > 0:
                    # è°ƒç”¨å¤„ç†æ¨è
                    processed_results = index_service.result_processor.process_recommendations(
                        documents=mock_retrieved_docs,
                        query=search_query,
                        top_k=min(len(mock_retrieved_docs), 3)
                    )
                    
                    # æ ¼å¼åŒ–æ¨èç»“æœ
                    answer = format_recommendation_results(processed_results)
                    
                    # æ·»åŠ å®æ—¶æŠ“å–æ ‡è®°
                    answer = f"## å®æ—¶æœç´¢ç»“æœ\n*ä»¥ä¸‹æ˜¯é€šè¿‡å®æ—¶æœç´¢è·å–çš„æœ€æ–°æ¨è*\n\n{answer}"
                else:
                    # æ²¡æœ‰è¶³å¤Ÿçš„æ–‡æ¡£ç”¨äºæ¨è
                    raise ValueError("æ²¡æœ‰è¶³å¤Ÿçš„æ–‡æ¡£ç”¨äºç”Ÿæˆæ¨è")
                    
            except Exception as e:
                logger.error(f"ä½¿ç”¨æ¨èå¤„ç†å™¨æ—¶å‡ºé”™: {str(e)}", exc_info=True)
                
                # å›é€€åˆ°ç®€å•çš„æ¨èæ ¼å¼
                answer = "# æ¨èç»“æœ (å®æ—¶æŠ“å–)\n\n"
                for i, post in enumerate(crawled_posts, 1):
                    title = post.thread_title if hasattr(post, 'thread_title') and post.thread_title else "æ— æ ‡é¢˜"
                    url = post.url if hasattr(post, 'url') and post.url else "#"
                    upvotes = post.upvotes if hasattr(post, 'upvotes') else 0
                    
                    # æå–æ‘˜è¦
                    content = post.content if hasattr(post, 'content') and post.content else ""
                    summary = content[:200] + "..." if len(content) > 200 else content
                    
                    answer += f"## {i}. **{title}**\n\n"
                    answer += f"- upvotes: {upvotes}\n"
                    answer += f"- [url]({url})\n\n"
                    answer += f"{summary}\n\n---\n\n"
            
            metadata = {
                'query_type': 'recommendation', 
                'processing': 'direct',
                'crawled': True,
                'real_time': True,
                'platform': platform,
                'query': search_query,
            }
            
            # å°†å¯¹è¯æ·»åŠ åˆ°è®°å¿†
            if session_id:
                MemoryService.add_to_memory(
                    session_id,
                    search_query,
                    answer
                )
            
            updated_memory = MemoryService.get_recent_memory(session_id) if session_id else []
            
            # ç›´æ¥è¿”å›ç»“æœï¼Œä¸ç»è¿‡LLMå¤„ç†
            return JsonResponse({
                'result': answer,
                'metadata': metadata,
                'llm_model': "recommendation_processor",
                'crawled': True,
                'history': updated_memory
            },
            json_dumps_params={'ensure_ascii': False}
        )

        
        # éæ¨èç±»æŸ¥è¯¢(2-6)ï¼Œä½¿ç”¨LLMå¤„ç†
        else:
            # æ„å»ºæç¤ºè¯ï¼Œç¡®ä¿ä¸generate_promptçš„æœŸæœ›æ ¼å¼ä¸€è‡´
            try:
                # åˆ›å»ºæ ‡å‡†çš„Documentå¯¹è±¡åˆ—è¡¨
                docs_for_prompt = []
                for post in crawled_posts:
                    doc = Document(
                        page_content=post.content,
                        metadata={
                            "source": platform,
                            "url": post.url,
                            "title": post.thread_title,
                            "id": f"rt-{i}",  # æ·»åŠ ä¸€ä¸ªå”¯ä¸€ID
                            "upvotes": post.upvotes if hasattr(post, 'upvotes') else 0
                        }
                    )
                    docs_for_prompt.append(doc)
                
                # ä½¿ç”¨æ ‡å‡†çš„generate_promptå‡½æ•°
                prompt = generate_prompt(
                    search_query, 
                    docs_for_prompt, 
                    recent_memory, 
                    platform, 
                    classification
                )
            except Exception as e:
                logger.error(f"ç”Ÿæˆæç¤ºè¯å¼‚å¸¸: {str(e)}", exc_info=True)
                # å¤‡ç”¨ç®€æ˜“æç¤ºè¯
                combined_content = ""
                for post in crawled_posts:
                    combined_content += f"æ ‡é¢˜: {post.thread_title}\n\n"
                    combined_content += f"å†…å®¹: {post.content}\n\n"
                    combined_content += f"URL: {post.url}\n\n"
                    combined_content += "---\n\n"
                
                prompt = f"""
åŸºäºä»¥ä¸‹ä»{platform}è·å–çš„æœ€æ–°å®æ—¶ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜:

ç”¨æˆ·é—®é¢˜: {search_query}

æœç´¢ç»“æœ:
{combined_content}

è¯·æ ¹æ®ä¸Šè¿°ä¿¡æ¯æä¾›å‡†ç¡®ã€æœ‰å¸®åŠ©çš„å›ç­”ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·å¦è¯šå‘ŠçŸ¥ã€‚
"""
            
            # å‘é€è‡³AI
            future = executor.submit(
                send_prompt, 
                prompt, 
                llm_model
            )
            response = future.result()
            
            # è§£æå“åº”
            try:
                answer, metadata = parse_langchain_response(response)
            except Exception as e:
                logger.error(f"è§£æå“åº”å¼‚å¸¸: {str(e)}", exc_info=True)
                # ç®€å•å“åº”
                answer = response if isinstance(response, str) else str(response)
                metadata = {}
            
            metadata.update({
                'crawled': True, 
                'real_time': True,
                'platform': platform, 
                'posts_count': len(crawled_posts)
            })
            
            # ä¿å­˜åˆ°è®°å¿†
            if session_id:
                MemoryService.add_to_memory(session_id, search_query, answer)
            
            updated_memory = MemoryService.get_recent_memory(session_id) if session_id else []
            
            return JsonResponse({
                'result': answer,
                'metadata': metadata,
                'llm_model': llm_model,
                'crawled': True,
                'history': updated_memory
            },
            json_dumps_params={'ensure_ascii': False}
        )

        
    except Exception as e:
        logger.error(f"å®æ—¶æŠ“å–å¼‚å¸¸: {str(e)}", exc_info=True)
        return JsonResponse({
            'result': f"æŠ±æ­‰ï¼Œåœ¨å°è¯•è·å–å…³äº'{search_query}'çš„æœ€æ–°ä¿¡æ¯æ—¶é‡åˆ°äº†é—®é¢˜ã€‚",
            'metadata': {'error': str(e), 'crawl_attempted': True},
            'llm_model': llm_model,
            'history': recent_memory
        })

@require_POST
def real_time_crawl(request):
    """
    å®æ—¶æŠ“å–ç«¯ç‚¹ï¼Œç”¨äºå‰ç«¯ç›´æ¥è°ƒç”¨å®æ—¶æŠ“å–åŠŸèƒ½ã€‚
    """
    data = json.loads(request.body.decode('utf-8'))
    search_query = data.get('search_query')
    platform = data.get('source', 'reddit')
    session_id = data.get('session_id')
    llm_model = data.get('llm_model', 'gemini-2.0-flash')
    
    if not search_query:
        return JsonResponse({'error': 'æœªæä¾›æœç´¢æŸ¥è¯¢'}, status=400)
    
    # è·å–è®°å¿†
    recent_memory = []
    if session_id:
        recent_memory = MemoryService.get_recent_memory(session_id, limit=10, platform=platform)
    
    # è°ƒç”¨å®æ—¶æŠ“å–å¤„ç†å‡½æ•°
    return handle_real_time_crawling(search_query, platform, session_id, llm_model, recent_memory)

def generate_xhs_search_url(query: str) -> str:
    """
    ä¿®æ­£ç‰ˆå°çº¢ä¹¦æœç´¢URLç”Ÿæˆå™¨

    :param query: ç”¨æˆ·è¾“å…¥çš„æœç´¢å…³é”®è¯ï¼ˆå¦‚ï¼š"æ–°å›½ç«‹"ï¼‰
    :return: ç¬¦åˆå°çº¢ä¹¦å®é™…è§„åˆ™çš„æœç´¢URL
    """
    # ç¬¬ä¸€æ¬¡ç¼–ç ï¼ˆä»…å¤„ç†ç‰¹æ®Šå­—ç¬¦ï¼‰
    primary_encoded = quote(query, safe='', encoding='utf-8')

    # é’ˆå¯¹ä¸­æ–‡è¿›è¡ŒäºŒæ¬¡ç¼–ç ï¼ˆä»…æ›¿æ¢%ä¸º%25ï¼‰
    double_encoded = primary_encoded.replace('%', '%25')

    # ç»„åˆæœ€ç»ˆURL
    return f"https://www.xiaohongshu.com/search_result?keyword={double_encoded}&source=web_search_result_notes"

