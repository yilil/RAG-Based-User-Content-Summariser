import logging
import re
import time
import json
from concurrent.futures import ThreadPoolExecutor
from django.shortcuts import render
from django.http import JsonResponse
from django.views.decorators.http import require_POST
from search_process.langchain_parser import parse_langchain_response
from search_process.prompt_generator import generate_prompt
from search_process.prompt_sender import send_prompt
from search_process.query_classification.classification import classify_query
from django_apps.memory.service import MemoryService
from django_apps.search.models import RedditContent, StackOverflowContent, RednoteContent, ContentIndex
from django_apps.search.index_service.base import IndexService
from django_apps.search.index_service.hybrid_retriever import HybridRetriever

# Initialize logger
logger = logging.getLogger(__name__)

# Initialize ThreadPoolExecutor
executor = ThreadPoolExecutor(max_workers=5)

# Initialize the shared index_service
index_service = IndexService(platform="reddit")  # Declare a global variable to hold the shared instance

def search(request):
    """
    å¤„ç†æœç´¢è¯·æ±‚ï¼š
    1. ä½¿ç”¨FAISSè¿›è¡Œç›¸ä¼¼æ–‡æ¡£æ£€ç´¢
    2. è°ƒç”¨Geminiå¤„ç†æœç´¢ç»“æžœ
    """
    print("Request Method:", request.method)
    print("Request Headers:", request.headers)
    print("Request Body:", request.body)
    global index_service

    logger.info("Received search request")
    logger.debug(f"Request method: {request.method}")
    logger.debug(f"POST data: {request.POST}")

    # é»˜è®¤è¿”å›žå˜é‡
    answer = ""
    metadata = {}
    retrieved_docs = []
    llm_model = ""

    # å¤„ç† POST è¯·æ±‚
    data = json.loads(request.body.decode('utf-8'))
    search_query = data.get('search_query')
    platform = data.get('source')
    filter_value = data.get('filter_value', None)
    llm_model = data.get('llm_model', llm_model)
    session_id = data.get('session_id')
    topic = data.get('topic')

    if not session_id:
        request.session.create()  # åˆ›å»ºæ–°ä¼šè¯
        session_id = request.session.session_key
    recent_memory = MemoryService.get_recent_memory(session_id, limit=10, platform=platform, topic=topic)

    if not search_query:
        return JsonResponse({
                'result': answer,
                'metadata': metadata,
                'llm_model': llm_model,
                'history': recent_memory
            })

    logger.info(f"Processing search query: {search_query} with model: {llm_model} and option: {filter_value}")

    try:
        #1. FAISSæœç´¢
        if not platform:
            platform = 'reddit'
        # æ›´æ–°å¹³å°å¹¶ç¡®ä¿åŠ è½½äº†å¯¹åº”çš„ç´¢å¼•
        index_service.platform = platform
        index_service.faiss_manager.set_platform(platform)

        index_count = index_service.faiss_manager.get_index_size()
        logger.info(f"å½“å‰{platform}å¹³å°ç´¢å¼•åŒ…å«{index_count}æ¡è®°å½•")

        # åˆå§‹åŒ– HybridRetriever
        hybrid_retriever = HybridRetriever(
            faiss_manager=index_service.faiss_manager,
            embedding_model=index_service.embedding_model,
            bm25_weight=0.3,  # å¯è°ƒæ•´çš„å‚æ•°
            embedding_weight=0.7,  # å¯è°ƒæ•´çš„å‚æ•°
            vote_weight=0  # å¯è°ƒæ•´çš„å‚æ•°
        )

        # èŽ·å–æœ€ç»ˆçš„ top_k retrieved_documents
        retrieved_docs = hybrid_retriever.retrieve(query=search_query, top_k=20, relevance_threshold=0.5) # æ·»åŠ é€‚å½“çš„é˜ˆå€¼

        logger.debug(f"Retrieved {len(retrieved_docs)} documents from FAISS")

        # 2. ç”Ÿæˆprompt
        classification = re.search(r">(\d+)<", classify_query(search_query, llm_model)).group(1)

        # *** -> å¦‚æžœæ˜¯æŽ¨èç±»æŸ¥è¯¢ï¼Œç›´æŽ¥ä½¿ç”¨process_recommendationså¤„ç† ***
        if classification == '1':  # æŽ¨èç±»æŸ¥è¯¢
            logger.info("ä½¿ç”¨æŽ¨èç±»å¤„ç†é€»è¾‘å¤„ç†æŸ¥è¯¢")
            
            # ä½¿ç”¨ResultProcessorå¤„ç†æŽ¨è -> è¿™é‡Œé¡µé¢çš„æ˜¾ç¤ºä¸Šè¿˜æœ‰é—®é¢˜ & è²Œä¼¼åªæœ‰è·‘mockæ•°æ®ï¼Œä½†æ˜¯retrieveåˆ°äº†æ–‡æ¡£
            processed_results = index_service.result_processor.process_recommendations(
                documents=retrieved_docs,
                query=search_query,
                top_k=20  # å¯é…ç½®çš„æŽ¨èæ•°é‡
            )
            
            # æ ¼å¼åŒ–æŽ¨èç»“æžœ
            answer = format_recommendation_results(processed_results)
            metadata = {'query_type': 'recommendation', 'processing': 'direct'}
            
            # å°†å¯¹è¯æ·»åŠ åˆ°è®°å¿†
            MemoryService.add_to_memory(
                session_id,
                search_query,
                answer
            )
            
            # ç›´æŽ¥è¿”å›žç»“æžœï¼Œä¸ç»è¿‡LLMå¤„ç†
            return JsonResponse({
                'result': answer,
                'metadata': metadata,
                'llm_model': "recommendation_processor",  # æ ‡è®°ä½¿ç”¨äº†æŽ¨èå¤„ç†å™¨
                'history': MemoryService.get_recent_memory(session_id)
            })

        # *** -> å¦‚æžœæ˜¯éžæŽ¨èç±»æŸ¥è¯¢ï¼Œèµ°æ­£å¸¸å¤„ç†é€»è¾‘ ***
        prompt = generate_prompt(search_query, retrieved_docs, recent_memory, platform, classification)

        future = executor.submit(
            send_prompt, 
            prompt, 
            llm_model
        )
        response = future.result()

        answer, metadata = parse_langchain_response(response)
        MemoryService.add_to_memory(session_id, search_query, answer)

    except Exception as e:
        logger.error(f"Error in search process: {str(e)}", exc_info=True)
        answer = "An unexpected error occurred. Please try again later."
        metadata = {}

    print(answer)
    recent_memory = MemoryService.get_recent_memory(session_id)
    return JsonResponse({
            'result': answer,
            'metadata': metadata,
            'llm_model': llm_model,
            'history': recent_memory
        })


# Initialization of indexing and embeddings
@require_POST
def index_content(request):
    """
    åˆå§‹åŒ–å†…å®¹ç´¢å¼•ï¼ˆå¢žé‡ç´¢å¼•ç‰ˆæœ¬ï¼‰ï¼š
      1. å¯¹æ¯ä¸ªå¹³å°ï¼Œå…ˆä»Žç£ç›˜åŠ è½½å·²æœ‰ç´¢å¼•
      2. æ‰¾å‡ºæ•°æ®åº“ä¸­æ–°çš„è®°å½•ï¼ˆä¸åœ¨ContentIndexçš„ï¼‰
      3. ä»…å¯¹æ–°è®°å½•åšembeddingå¹¶å†™å…¥å†…å­˜ç´¢å¼•
      4. ä¿å­˜åˆå¹¶åŽçš„ç´¢å¼•åˆ°æœ¬åœ°ç£ç›˜
    """

    global index_service

    results = {}

    start_time = time.time()
    logger.info("Starting content indexing process (incremental).")

    source_filter = request.POST.get('source')
    if not source_filter:
        platforms = ['reddit', 'stackoverflow', 'rednote']
    else:
        platforms = [source_filter]

    try:
        for platform in platforms:
            logger.info(f"Processing platform: {platform}")
            if platform not in ['reddit', 'stackoverflow', 'rednote']:
                logger.warning(f"Unknown platform: {platform}, skip.")
                continue

            # æ›´æ–°å¹³å°
            index_service.platform = platform
            index_service.faiss_manager.set_platform(platform)
            
            # å°è¯•åŠ è½½å·²æœ‰ç´¢å¼•
            try:
                loaded = index_service.faiss_manager.load_index()
                if not loaded:
                    logger.info(f"æ²¡æœ‰æ‰¾åˆ°çŽ°æœ‰ç´¢å¼•ï¼Œä¸º {platform} åˆ›å»ºæ–°ç´¢å¼•")
                    index_service.faiss_manager.create_empty_index()
                    # ç¡®ä¿ä¿å­˜ç©ºç´¢å¼•
                    index_service.faiss_manager.save_index()
                    logger.info(f"ç©ºç´¢å¼•å·²åˆ›å»ºå¹¶ä¿å­˜åˆ°ç£ç›˜ï¼š{platform}")
            except Exception as e:
                logger.error(f"åŠ è½½æˆ–åˆ›å»ºç´¢å¼•å¤±è´¥: {str(e)}")
                results[platform] = f"error: {str(e)}"
                continue

            # æ ¹æ®å¹³å°èŽ·å–"æœªç´¢å¼•çš„"æ–°å†…å®¹
            if platform == 'reddit':
                model_cls = RedditContent
            elif platform == 'stackoverflow':
                model_cls = StackOverflowContent
            else:  # 'rednote'
                model_cls = RednoteContent

            # *** ç›®å‰æ”¹æˆç”¨threadidæ¥åˆ¤æ–­æ˜¯å¦é‡å¤
            unindexed = model_cls.objects.exclude(
            thread_id__in=ContentIndex.objects.filter(source=platform).values('thread_id')
            )   

            count_unindexed = unindexed.count()

            if count_unindexed > 0:
                logger.info(f"Found {count_unindexed} new {platform} items to index.")
                # è°ƒç”¨ index_platform_content() å†…éƒ¨ä¼šï¼š
                # 1) ä»…å¯¹ unindexed æ•°æ®åš embedding + add_texts
                # 2) å°†å…¶å†™å…¥ ContentIndex
                # 3) è°ƒç”¨ save_index() å†å†™å›žç£ç›˜
                index_service.indexer.index_platform_content(platform=platform, unindexed_queryset=unindexed)
                # æ·»åŠ å†…å®¹åŽä¿å­˜ç´¢å¼•
                index_service.faiss_manager.save_index()
                logger.info(f"Saved FAISS index for {platform}")
            else:
                logger.info(f"No new {platform} items to index.")

        duration = time.time() - start_time
        logger.info(f"Indexing completed in {duration:.2f} seconds.")
        return JsonResponse({
            "status": "success",
            "message": "Incremental indexing completed successfully",
            "duration": round(duration, 2)
        })

    except Exception as e:
        logger.error(f"Indexing failed: {str(e)}")
        return JsonResponse({
            "status": "error",
            "message": str(e)
        }, status=500)
        
def sessionKey(request):
    session_id = request.session.session_key
    if not session_id:
        request.session.create()  # åˆ›å»ºæ–°ä¼šè¯
        session_id = request.session.session_key
    data = json.loads(request.body.decode('utf-8'))
    platform = data.get('platform')
    topic = data.get('topic')
    MemoryService.get_or_create_memory(session_id, platform, topic)
    return JsonResponse({
        'session_id': session_id
    })

def getMemory(request):
    session_id = request.GET.get('session_id')
    if not session_id:
        return JsonResponse({
            'error': 'session_id is required'
        }, status=400)
    memory = MemoryService.get_recent_memory(session_id)
    return JsonResponse({
        'memory': memory
    })

def getAllChat(request):
    sessions = MemoryService.get_all_sessions()
    sessions_data = [
        {
            "session_id": s.session_id,
            "platform": s.platform,
            "topic": s.topic,
            "memory_data": s.memory_data,
            "updated_at": s.updated_at.isoformat()  # DateTime éœ€è¦è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        }
        for s in sessions
    ]
    return JsonResponse({
        'sessions': sessions_data
    })


def format_recommendation_results(results):
    """å°†æŽ¨èå¤„ç†ç»“æžœæ ¼å¼åŒ–ä¸ºå¯è¯»æ–‡æœ¬"""
    if not results:
        return "æœªæ‰¾åˆ°ç›¸å…³æŽ¨èã€‚"
        
    formatted_text = "# æ ¹æ®æ‚¨çš„æŸ¥è¯¢ï¼Œä¸ºæ‚¨æŽ¨èä»¥ä¸‹é€‰é¡¹ï¼š\n\n"
    
    for doc in results:
        metadata = doc.metadata
        formatted_text += f"## {metadata['name']}\n"
        formatted_text += f"- è¯„åˆ†: {metadata['avg_rating']:.1f}/5.0 ({metadata['mentions']} æ¡è¯„è®º)\n"
        formatted_text += f"- äººæ°”: {metadata['total_upvotes']} ç‚¹èµž\n"
        formatted_text += f"- æ‘˜è¦: {metadata['summary']}\n\n"
        
        formatted_text += "### ç”¨æˆ·è¯„ä»·:\n"
        for post in metadata['posts'][:3]:  # æœ€å¤šæ˜¾ç¤º3æ¡è¯„è®º
            rating = int(round(post['rating']))
            if rating > 5: rating = 5
            if rating < 1: rating = 1
            sentiment = "éžå¸¸æ­£é¢" if rating == 5 else "æ­£é¢" if rating == 4 else "ä¸­æ€§" if rating == 3 else "è´Ÿé¢" if rating == 2 else "éžå¸¸è´Ÿé¢"
            
            formatted_text += f"- {post['content']}\n"
            formatted_text += f"  ({sentiment}, {post['upvotes']} ç‚¹èµž)\n"
        
        formatted_text += "\n"
        
    # æ·»åŠ æ¯”è¾ƒè¡¨æ ¼
    formatted_text += "## æ¯”è¾ƒè¡¨\n\n"
    formatted_text += "| åç§° | è¯„åˆ† | äººæ°” | ç»¼åˆå¾—åˆ† | æŽ¨èæŒ‡æ•° |\n"
    formatted_text += "|------|------|------|----------|----------|\n"

    for doc in results:
        metadata = doc.metadata
        stars = "â­" * int(round(metadata['avg_rating']))
        formatted_text += f"| {metadata['name']} | {metadata['avg_rating']:.1f} {stars} | {metadata['total_upvotes']} | {metadata['score']:.2f} | {'ðŸ”¥' * (6 - metadata['rank'])} |\n"
    
    return formatted_text