import logging
import re
import time
import json
from concurrent.futures import ThreadPoolExecutor
from django.shortcuts import render
from django.http import JsonResponse
from django.views.decorators.http import require_POST
from search_process.langchain_parser import parse_langchain_response
from search_process.prompt_generator import generate_prompt
from search_process.prompt_sender import send_prompt
from search_process.query_classification.classification import classify_query
from django_apps.memory.service import MemoryService
from django_apps.search.models import RedditContent, StackOverflowContent, RednoteContent, ContentIndex
from django_apps.search.index_service.base import IndexService
from django_apps.search.index_service.hybrid_retriever import HybridRetriever
from typing import List, Dict
from langchain.docstore.document import Document
from urllib.parse import quote
from django_apps.search.crawler_config import REDNOTE_LOGIN_COOKIES

# Initialize logger
logger = logging.getLogger(__name__)

# Initialize ThreadPoolExecutor
executor = ThreadPoolExecutor(max_workers=5)

# Initialize the shared index_service
index_service = IndexService(platform="reddit")  # Declare a global variable to hold the shared instance

def search(request):
    """
    å¤„ç†æœç´¢è¯·æ±‚ï¼š
    1. ä½¿ç”¨FAISSè¿›è¡Œç›¸ä¼¼æ–‡æ¡£æ£€ç´¢
    2. å¦‚æœæ²¡æœ‰ç›¸å…³ç»“æœ & å¼€å¯äº†å®æ—¶æŠ“å–ï¼Œåˆ™è°ƒç”¨å®æ—¶æŠ“å–
    3. è°ƒç”¨Geminiå¤„ç†æœç´¢ç»“æœ
    """
    print("Request Method:", request.method)
    print("Request Headers:", request.headers)
    print("Request Body:", request.body)
    global index_service

    logger.info("Received search request")
    logger.debug(f"Request method: {request.method}")
    logger.debug(f"POST data: {request.POST}")

    # é»˜è®¤è¿”å›å˜é‡
    answer = ""
    metadata = {}
    retrieved_docs = []
    llm_model = ""

    # å¤„ç† POST è¯·æ±‚
    data = json.loads(request.body.decode('utf-8'))
    search_query = data.get('search_query')
    platform = data.get('source')
    filter_value = data.get('filter_value', None)
    llm_model = data.get('llm_model', llm_model)
    session_id = data.get('session_id')
    topic = data.get('topic')
    
    # è·å–å®æ—¶æŠ“å–è®¾ç½®
    real_time_crawling_enabled = data.get('real_time_crawling_enabled', False)

    if not session_id:
        request.session.create()  # åˆ›å»ºæ–°ä¼šè¯
        session_id = request.session.session_key
    recent_memory = MemoryService.get_recent_memory(session_id, limit=10, platform=platform, topic=topic)

    if not search_query:
        return JsonResponse({
                'result': answer,
                'metadata': metadata,
                'llm_model': llm_model,
                'history': recent_memory
            },
            json_dumps_params={'ensure_ascii': False}
        )

    logger.info(f"Processing search query: {search_query} with model: {llm_model} and option: {filter_value}")

    try:
        #1. FAISSæœç´¢
        if not platform:
            platform = 'reddit'
        # æ›´æ–°å¹³å°å¹¶ç¡®ä¿åŠ è½½äº†å¯¹åº”çš„ç´¢å¼•
        index_service.platform = platform
        index_service.faiss_manager.set_platform(platform)

        index_count = index_service.faiss_manager.get_index_size()
        logger.info(f"å½“å‰{platform}å¹³å°ç´¢å¼•åŒ…å«{index_count}æ¡è®°å½•")

        # åˆå§‹åŒ– HybridRetriever
        hybrid_retriever = HybridRetriever(
            faiss_manager=index_service.faiss_manager,
            embedding_model=index_service.embedding_model,
            bm25_weight=0.55,
            embedding_weight=0.35,
            vote_weight=0.1,
            l2_decay_beta=6.0
        )

        # è·å–æœ€ç»ˆçš„ top_k retrieved_documents
        print(f"--- [views.search] è°ƒç”¨ hybrid_retriever.retrieve (Query: '{search_query}')... ---")
        retrieved_docs = hybrid_retriever.retrieve(query=search_query, top_k=5, relevance_threshold=0.7) # å¯ä»¥åŠ¨æ€è°ƒæ•´
        print(f"--- [views.search] hybrid_retriever.retrieve è¿”å›äº† {len(retrieved_docs)} ä¸ªæ–‡æ¡£ ---")

        # --- å…³é”®æ‰“å°ï¼šæ£€æŸ¥ä¼ é€’ç»™ generate_prompt çš„æ–‡æ¡£å…ƒæ•°æ® ---
        print(f"--- [views.search] å‡†å¤‡è°ƒç”¨ generate_prompt, æ£€æŸ¥ retrieved_docs å…ƒæ•°æ® (Top 5): ---")
        for i, doc in enumerate(retrieved_docs[:5]):
            if hasattr(doc, 'metadata'):
                print(f"  Doc {i+1}: Metadata = {doc.metadata}")
            else:
                print(f"  Doc {i+1}: Error - Document has no metadata attribute.")
        # --- ç»“æŸå…³é”®æ‰“å° ---

        # 2. ç”Ÿæˆprompt
        classification = re.search(r">(\d+)<", classify_query(search_query, llm_model)).group(1)

        # æ£€æŸ¥æ˜¯å¦æœ‰æœç´¢ç»“æœï¼Œå¦‚æœæ²¡æœ‰ä¸”å¼€å¯äº†å®æ—¶æŠ“å–ï¼Œåˆ™è°ƒç”¨æ··åˆæœç´¢
        if not retrieved_docs or len(retrieved_docs) < 1: # æ”¹top_kçš„æ—¶å€™æ³¨æ„è¿™é‡Œ
            print(f"æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç»“æœ: {search_query}")
            logger.info(f"æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç»“æœ: {search_query}")
            
            # å¦‚æœå¼€å¯äº†å®æ—¶æŠ“å–åŠŸèƒ½ï¼Œè°ƒç”¨æ··åˆæœç´¢
            if real_time_crawling_enabled:
                logger.info(f"æ··åˆæœç´¢å·²å¯ç”¨ï¼Œå¼€å§‹ä¸ºæŸ¥è¯¢æŠ“å–: {search_query}")
                return handle_mixed_search(
                    search_query, 
                    platform, 
                    session_id, 
                    llm_model, 
                    recent_memory,
                    classification
                )
            
            # å¦‚æœæ²¡æœ‰å¼€å¯å®æ—¶æŠ“å–ï¼Œè¿”å›æ— ç»“æœæç¤º
            answer = f"æŠ±æ­‰ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°å…³äº'{search_query}'çš„ç›¸å…³ä¿¡æ¯ã€‚è¯•è¯•å¼€å¯å®æ—¶æœç´¢è·å–æœ€æ–°ç»“æœã€‚"
            metadata = {'no_results': True}
            
            return JsonResponse({
                'result': answer,
                'metadata': metadata,
                'llm_model': llm_model,
                'history': recent_memory
            },
            json_dumps_params={'ensure_ascii': False}
        )


        # *** -> å¦‚æœæ˜¯æ¨èç±»æŸ¥è¯¢ï¼Œç›´æ¥ä½¿ç”¨process_recommendationså¤„ç† ***
        if classification == '1':  # æ¨èç±»æŸ¥è¯¢
            logger.info("ä½¿ç”¨æ¨èç±»å¤„ç†é€»è¾‘å¤„ç†æŸ¥è¯¢")
            
            # ä½¿ç”¨ResultProcessorå¤„ç†æ¨è -> è¿™é‡Œé¡µé¢çš„æ˜¾ç¤ºä¸Šè¿˜æœ‰é—®é¢˜ & è²Œä¼¼åªæœ‰è·‘mockæ•°æ®ï¼Œä½†æ˜¯retrieveåˆ°äº†æ–‡æ¡£
            top_for_prompt = sorted(
                retrieved_docs,
                key=lambda d: d.metadata.get('relevance_score', 0),
                reverse=True
            )[:5]
            processed_results = index_service.result_processor.process_recommendations(
                documents=top_for_prompt,
                query=search_query,
                top_k=5  # å¯é…ç½®çš„æ¨èæ•°é‡
            )
            
            # æ ¼å¼åŒ–æ¨èç»“æœ
            answer = format_recommendation_results(processed_results)
            metadata = {'query_type': 'recommendation', 'processing': 'direct'}
            
            # å°†å¯¹è¯æ·»åŠ åˆ°è®°å¿†
            MemoryService.add_to_memory(
                session_id,
                search_query,
                answer
            )
            
            # ç›´æ¥è¿”å›ç»“æœï¼Œä¸ç»è¿‡LLMå¤„ç†
            return JsonResponse({
                'result': answer,
                'metadata': metadata,
                'llm_model': "recommendation_processor",  # æ ‡è®°ä½¿ç”¨äº†æ¨èå¤„ç†å™¨
                'history': MemoryService.get_recent_memory(session_id)
            },
            json_dumps_params={'ensure_ascii': False}
        )


        # *** -> å¦‚æœæ˜¯éæ¨èç±»æŸ¥è¯¢ï¼Œèµ°æ­£å¸¸å¤„ç†é€»è¾‘ ***
        prompt = generate_prompt(search_query, retrieved_docs, recent_memory, platform, classification)

        future = executor.submit(
            send_prompt, 
            prompt, 
            llm_model
        )
        response = future.result()

        answer, metadata = parse_langchain_response(response)
        MemoryService.add_to_memory(session_id, search_query, answer)

    except Exception as e:
        logger.error(f"Error in search process: {str(e)}", exc_info=True)
        answer = "An unexpected error occurred. Please try again later."
        metadata = {}

    print(answer)
    recent_memory = MemoryService.get_recent_memory(session_id)
    return JsonResponse({
            'result': answer,
            'metadata': metadata,
            'llm_model': llm_model,
            'history': recent_memory
        },
            json_dumps_params={'ensure_ascii': False}
        )



# Initialization of indexing and embeddings
@require_POST
def index_content(request):
    """
    åˆå§‹åŒ–å†…å®¹ç´¢å¼•ï¼ˆå¢é‡ç´¢å¼•ç‰ˆæœ¬ï¼‰ï¼š
      1. å¯¹æ¯ä¸ªå¹³å°ï¼Œå…ˆä»ç£ç›˜åŠ è½½å·²æœ‰ç´¢å¼•
      2. æ‰¾å‡ºæ•°æ®åº“ä¸­æ–°çš„è®°å½•ï¼ˆä¸åœ¨ContentIndexçš„ï¼‰
      3. ä»…å¯¹æ–°è®°å½•åšembeddingå¹¶å†™å…¥å†…å­˜ç´¢å¼•
      4. ä¿å­˜åˆå¹¶åçš„ç´¢å¼•åˆ°æœ¬åœ°ç£ç›˜
    """

    global index_service

    results = {}

    start_time = time.time()
    logger.info("Starting content indexing process (incremental).")

    source_filter = request.POST.get('source')
    if not source_filter:
        platforms = ['reddit', 'stackoverflow', 'rednote']
    else:
        platforms = [source_filter]

    try:
        for platform in platforms:
            logger.info(f"Processing platform: {platform}")
            if platform not in ['reddit', 'stackoverflow', 'rednote']:
                logger.warning(f"Unknown platform: {platform}, skip.")
                continue

            # æ›´æ–°å¹³å°
            index_service.platform = platform
            index_service.faiss_manager.set_platform(platform)
            
            # å°è¯•åŠ è½½å·²æœ‰ç´¢å¼•
            try:
                loaded = index_service.faiss_manager.load_index()
                if not loaded:
                    logger.info(f"æ²¡æœ‰æ‰¾åˆ°ç°æœ‰ç´¢å¼•ï¼Œä¸º {platform} åˆ›å»ºæ–°ç´¢å¼•")
                    index_service.faiss_manager.create_empty_index()
                    # ç¡®ä¿ä¿å­˜ç©ºç´¢å¼•
                    index_service.faiss_manager.save_index()
                    logger.info(f"ç©ºç´¢å¼•å·²åˆ›å»ºå¹¶ä¿å­˜åˆ°ç£ç›˜ï¼š{platform}")
            except Exception as e:
                logger.error(f"åŠ è½½æˆ–åˆ›å»ºç´¢å¼•å¤±è´¥: {str(e)}")
                results[platform] = f"error: {str(e)}"
                continue

            # æ ¹æ®å¹³å°è·å–"æœªç´¢å¼•çš„"æ–°å†…å®¹
            if platform == 'reddit':
                model_cls = RedditContent
            elif platform == 'stackoverflow':
                model_cls = StackOverflowContent
            else:  # 'rednote'
                model_cls = RednoteContent

            # *** ç›®å‰æ”¹æˆç”¨threadidæ¥åˆ¤æ–­æ˜¯å¦é‡å¤
            unindexed = model_cls.objects.exclude(
            thread_id__in=ContentIndex.objects.filter(source=platform).values('thread_id')
            )   

            count_unindexed = unindexed.count()

            if count_unindexed > 0:
                logger.info(f"Found {count_unindexed} new {platform} items to index.")
                # è°ƒç”¨ index_platform_content() å†…éƒ¨ä¼šï¼š
                # 1) ä»…å¯¹ unindexed æ•°æ®åš embedding + add_texts
                # 2) å°†å…¶å†™å…¥ ContentIndex
                # 3) è°ƒç”¨ save_index() å†å†™å›ç£ç›˜
                index_service.indexer.index_platform_content(platform=platform, unindexed_queryset=unindexed)
                # æ·»åŠ å†…å®¹åä¿å­˜ç´¢å¼•
                # index_service.faiss_manager.save_index()
                logger.info(f"Saved FAISS index for {platform}")
            else:
                logger.info(f"No new {platform} items to index.")

        duration = time.time() - start_time
        logger.info(f"Indexing completed in {duration:.2f} seconds.")
        return JsonResponse({
            "status": "success",
            "message": "Incremental indexing completed successfully",
            "duration": round(duration, 2)
        })

    except Exception as e:
        logger.error(f"Indexing failed: {str(e)}")
        return JsonResponse({
            "status": "error",
            "message": str(e)
        }, status=500)
        
def sessionKey(request):
    session_id = request.session.session_key
    if not session_id:
        request.session.create()  # åˆ›å»ºæ–°ä¼šè¯
        session_id = request.session.session_key
    data = json.loads(request.body.decode('utf-8'))
    platform = data.get('platform')
    topic = data.get('topic')
    MemoryService.get_or_create_memory(session_id, platform, topic)
    return JsonResponse({
        'session_id': session_id
    })

def getMemory(request):
    session_id = request.GET.get('session_id')
    if not session_id:
        return JsonResponse({
            'error': 'session_id is required'
        }, status=400)
    memory = MemoryService.get_recent_memory(session_id)
    return JsonResponse({
        'memory': memory
    })

def getAllChat(request):
    sessions = MemoryService.get_all_sessions()
    sessions_data = [
        {
            "session_id": s.session_id,
            "platform": s.platform,
            "topic": s.topic,
            "memory_data": s.memory_data,
            "updated_at": s.updated_at.isoformat()  # DateTime éœ€è¦è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        }
        for s in sessions
    ]
    return JsonResponse({
        'sessions': sessions_data
    })


def format_recommendation_results(results: List[Document]) -> str:
    if not results:
        return "<p>æœªæ‰¾åˆ°ç›¸å…³æ¨èã€‚</p>"

    # Introductory text - outside any card
    output_lines = ["<p>æ ¹æ®æ‚¨çš„æŸ¥è¯¢ï¼Œä¸ºæ‚¨æ¨èä»¥ä¸‹é€‰é¡¹ï¼š</p>"]

    # ---- Start: Main Card for ALL Recommendation Details ----
    output_lines.append('<div class="recommendation-details-card">')

    for i, doc in enumerate(results, 1):
        m = doc.metadata
        name = m.get('name', 'N/A').replace('\\n', ' ')
        avg_rating = m.get('avg_rating', 0.0)
        total_upvotes = m.get('total_upvotes', 0)
        mentions = m.get('mentions', 0)
        sentiment_counts = m.get('sentiment_counts', {})

        if i > 1:
            output_lines.append('<hr style="margin: 20px 0; border-top: 1px solid #eee;">')

        output_lines.append(f'<h4 style="margin-bottom: 8px; font-size: 1.15em;">{i}. {name}</h4>')

        # Rating line with emojis exactly as in reference image
        rating_display = f"{avg_rating:.1f}/5.0"
        output_lines.append(
            f'<p style="font-size: 0.9em; color: #555; margin-top: 8px; margin-bottom: 8px;">'
            f'<span>â­ Rating: {rating_display}</span> '
            f'<span>ğŸ‘ Upvotes: {total_upvotes}</span> '
            f'<span>ğŸ“ Mentions: {mentions}</span>'
            f'</p>'
        )

        # Sentiment line with proper spacing and spans for color blocks
        output_lines.append(
            f'<p style="font-size: 0.9em; color: #555; margin-bottom: 12px;">'
            f"Sentiment: <span class='sentiment-positive'>Positive: {sentiment_counts.get('positive', 0)}</span>, "
            f"<span class='sentiment-neutral'>Neutral: {sentiment_counts.get('neutral', 0)}</span>, "
            f"<span class='sentiment-negative'>Negative: {sentiment_counts.get('negative', 0)}</span>"
            f'</p>' 
        )

        # General Summary section with proper styling
        output_lines.append("<p style='margin-top: 15px; font-weight: bold;'>General Summary:</p>")
        output_lines.append("<ul>")
        general_summary_text = m.get('summary', 'No summary available.')
        summary_points = [s.strip() for s in general_summary_text.split('\n') if s.strip()]
        if not summary_points and general_summary_text.strip():
            summary_points = [general_summary_text.strip()]
        
        if not summary_points:
            output_lines.append("<li>No summary available.</li>")
        else:
            for point in summary_points:
                output_lines.append(f"<li>{point}</li>")
        output_lines.append("</ul>")

        # Detailed Reviews section with proper styling
        output_lines.append("<p style='margin-top: 15px; font-weight: bold;'>Detailed Reviews:</p>")
        output_lines.append("<ul>")
        posts = m.get('posts', [])
        if not posts:
            output_lines.append("<li>No detailed reviews available.</li>")
        else:
            for review_item in posts[:3]:
                content = review_item.get('content', 'N/A').replace('\n', '<br />')
                original_upvotes = review_item.get('upvotes', 0)
                # Use exact emoji format as in reference
                output_lines.append(f"<li>{content} (ğŸ‘{original_upvotes})</li>")
        output_lines.append("</ul>")

    output_lines.append('</div>')

    if results:
        # Add comparison table section with proper heading and styling
        output_lines.append('<div class="comparison-table-card">')
        output_lines.append('<h3><strong>Comparison Table</strong></h3>')
        
        # Full HTML table with proper border attributes and width
        output_lines.append('<table class="comparison-table" border="1" cellspacing="0" cellpadding="0" style="width:100%;">')
        
        # Table header row
        output_lines.append('<thead><tr>')
        output_lines.append('<th>Name</th>')
        output_lines.append('<th>Avg. Rating</th>')
        output_lines.append('<th>Total Upvotes</th>')
        output_lines.append('<th>Mentions</th>')
        output_lines.append('</tr></thead>')
        
        # Table body
        output_lines.append('<tbody>')
        for doc_table in results:
            m_table = doc_table.metadata
            name_table = m_table.get('name', 'N/A').replace('\\n', ' ')
            avg_rating_val = m_table.get('avg_rating', 0.0)
            total_upvotes = m_table.get("total_upvotes", 0)
            mentions = m_table.get("mentions", 0)
            
            output_lines.append(
                f'<tr><td>{name_table}</td><td>{avg_rating_val:.1f}</td><td>{total_upvotes}</td><td>{mentions}</td></tr>'
            )
        output_lines.append('</tbody>')
        output_lines.append('</table>')
        output_lines.append('</div>')

    return "".join(output_lines)

def handle_mixed_search(search_query, platform, session_id, llm_model, recent_memory, classification=None):
    """
    å¤„ç†æ··åˆæœç´¢åŠŸèƒ½çš„è¾…åŠ©å‡½æ•°ã€‚
    å…ˆå°è¯•ä»æ•°æ®åº“æœç´¢ï¼Œå¦‚æœæ— ç»“æœåˆ™è¿›è¡Œå®æ—¶æŠ“å–ã€‚
    """
    logger.info(f"å¼€å§‹æ··åˆæœç´¢: {search_query}, å¹³å°: {platform}")
    
    try:
        # å¦‚æœåˆ†ç±»æœªæä¾›ï¼Œè¿›è¡Œåˆ†ç±»
        if classification is None:
            classification = re.search(r">(\d+)<", classify_query(search_query, llm_model)).group(1)
            logger.info(f"æŸ¥è¯¢'{search_query}'è¢«åˆ†ç±»ä¸º: {classification}")
        
        # æ ¹æ®å¹³å°é€‰æ‹©é€‚å½“çš„çˆ¬è™«
        if platform == 'reddit':
            try:
                from django_apps.search.reddit_crawler import create_reddit_instance, fetch_and_store_reddit_posts
                
                # åˆ›å»ºRedditå®ä¾‹
                reddit = create_reddit_instance()
                
                # ä¼˜åŒ–æŸ¥è¯¢è¯
                query_words = search_query.split()
                if len(query_words) > 3:
                    # ç§»é™¤å¸¸è§åœç”¨è¯
                    stopwords = {'a', 'an', 'the', 'is', 'are', 'was', 'were', 'what', 'which', 'how', 'why', 'when', 'who'}
                    optimized_query = ' '.join([word for word in query_words if word.lower() not in stopwords])
                else:
                    optimized_query = search_query
                    
                # å¯¹äºæ¨èç±»æŸ¥è¯¢æ·»åŠ æœç´¢ä¿®é¥°ç¬¦
                if 'recommend' in search_query.lower() or 'suggest' in search_query.lower() or classification == '1':
                    optimized_query += ' recommend OR suggest OR best'
                
                logger.info(f"ä¼˜åŒ–åçš„RedditæŸ¥è¯¢: {optimized_query}")
                
                # æŠ“å–å¸–å­
                crawled_posts = fetch_and_store_reddit_posts(reddit, optimized_query, limit=5)
                
                # åœ¨åå°å¤„ç†æ•°æ®ï¼Œè¿›è¡Œembedding
                import threading
                threading.Thread(
                    target=process_crawled_data_for_indexing,
                    args=(crawled_posts, platform),
                    daemon=True
                ).start()
                
            except Exception as e:
                logger.error(f"Redditçˆ¬è™«å¼‚å¸¸: {str(e)}", exc_info=True)
                return JsonResponse({
                    'result': f"æŠ±æ­‰ï¼Œåœ¨æŠ“å–Redditä¿¡æ¯æ—¶é‡åˆ°é—®é¢˜: {str(e)}",
                    'metadata': {'crawler_error': str(e)},
                    'llm_model': llm_model,
                    'history': recent_memory
                })
            
        elif platform == 'stackoverflow':
            try:
                from django_apps.search.stackoverflow_crawler import fetch_and_store_stackoverflow_data
                
                # å¯¹äºStackOverflowä¸éœ€è¦å¤ªå¤šä¼˜åŒ–
                crawled_posts = fetch_and_store_stackoverflow_data(search_query, limit=5)
                
                # åœ¨åå°å¤„ç†æ•°æ®ï¼Œè¿›è¡Œembedding
                import threading
                threading.Thread(
                    target=process_crawled_data_for_indexing,
                    args=(crawled_posts, platform),
                    daemon=True
                ).start()
                
            except Exception as e:
                logger.error(f"StackOverflowçˆ¬è™«å¼‚å¸¸: {str(e)}", exc_info=True)
                return JsonResponse({
                    'result': f"æŠ±æ­‰ï¼Œåœ¨æŠ“å–StackOverflowä¿¡æ¯æ—¶é‡åˆ°é—®é¢˜: {str(e)}",
                    'metadata': {'crawler_error': str(e)},
                    'llm_model': llm_model,
                    'history': recent_memory
                })
            
        elif platform == 'rednote':
            try:
                from django_apps.search.crawler import crawl_rednote_page

                target_url = generate_xhs_search_url(search_query)
                logger.info(f"Generated RedNote search URL: {target_url}")

                # ä½¿ç”¨ä» crawler_config å¯¼å…¥çš„ Cookies
                cookies_to_use = REDNOTE_LOGIN_COOKIES
                if not cookies_to_use:
                     raise ValueError("RedNote cookies not configured or empty in crawler_config.py.")

                crawled_posts = crawl_rednote_page(url=target_url, cookies=cookies_to_use, immediate_indexing=False)
                logger.info(f"RedNote crawler attempted search URL, found {len(crawled_posts)} potential posts.")
                
                # åœ¨åå°å¤„ç†æ•°æ®ï¼Œè¿›è¡Œembedding
                import threading
                threading.Thread(
                    target=process_crawled_data_for_indexing,
                    args=(crawled_posts, platform),
                    daemon=True
                ).start()

            except ImportError:
                 logger.error(f"RedNote crawler function not found.")
                 return JsonResponse({
                    'result': f"æŠ±æ­‰ï¼ŒRedNoteå®æ—¶æŠ“å–åŠŸèƒ½é…ç½®é”™è¯¯ã€‚",
                    'metadata': {'crawler_error': 'ImportError', 'crawl_attempted': True, 'platform': platform},
                    'llm_model': llm_model,
                    'history': recent_memory
                })
            except Exception as e:
                logger.error(f"RedNoteçˆ¬è™«å¼‚å¸¸ (URL: {target_url}): {str(e)}", exc_info=True) # Log URL
                return JsonResponse({
                    'result': f"æŠ±æ­‰ï¼Œåœ¨æŠ“å–RedNoteä¿¡æ¯æ—¶é‡åˆ°é—®é¢˜: {str(e)}",
                    'metadata': {'crawler_error': str(e), 'crawl_attempted': True, 'platform': platform},
                    'llm_model': llm_model,
                    'history': recent_memory
                })
        else:
            return JsonResponse({
                'result': f"æŠ±æ­‰ï¼Œå¹³å°'{platform}'ä¸æ”¯æŒå®æ—¶æŠ“å–åŠŸèƒ½ã€‚",
                'metadata': {'error': 'unsupported_platform'},
                'llm_model': llm_model,
                'history': recent_memory
            })
        
        if not crawled_posts:
            logger.warning(f"æœªæ‰¾åˆ°æŸ¥è¯¢ç»“æœ: {search_query}, å¹³å°: {platform}")
            return JsonResponse({
                'result': f"æŠ±æ­‰ï¼Œæˆ‘æ— æ³•åœ¨{platform}æ‰¾åˆ°å…³äº'{search_query}'çš„ç›¸å…³ä¿¡æ¯ã€‚",
                'metadata': {'no_results': True, 'crawl_attempted': True},
                'llm_model': llm_model,
                'history': recent_memory
            })
        
        logger.info(f"æˆåŠŸæŠ“å– {len(crawled_posts)} æ¡å†…å®¹ï¼ŒæŸ¥è¯¢: {search_query}, å¹³å°: {platform}")
        
        # å¦‚æœæ˜¯æ¨èç±»æŸ¥è¯¢(1)ï¼Œåˆ™ä½¿ç”¨ç›´æ¥å¤„ç†æ–¹å¼
        if classification == '1':
            logger.info("ä½¿ç”¨æ¨èç±»å¤„ç†é€»è¾‘å¤„ç†å®æ—¶æŠ“å–ç»“æœ")
            
            # åˆ›å»ºæ ‡å‡†æ ¼å¼çš„æ–‡æ¡£å¯¹è±¡
            mock_retrieved_docs = []
            for post in crawled_posts:
                # åˆ›å»ºç¬¦åˆDocumentæ ¼å¼çš„å¯¹è±¡
                mock_doc = Document(
                    page_content=post.content,
                    metadata={
                        "source": platform,
                        "score": post.upvotes if hasattr(post, 'upvotes') else 0,
                        "url": post.url,
                        "title": post.thread_title,
                        "thread_id": post.thread_id,
                        "author": post.author_name,
                        "upvotes": post.upvotes if hasattr(post, 'upvotes') else 0
                    }
                )
                mock_retrieved_docs.append(mock_doc)
            
            # ä½¿ç”¨ResultProcessorå¤„ç†æ¨è
            try:
                # å°è¯•ä½¿ç”¨ResultProcessorï¼Œä½†å‡†å¤‡å¥½ç›´æ¥å¤‡ç”¨æ–¹æ¡ˆ
                if len(mock_retrieved_docs) > 0:
                    # è°ƒç”¨å¤„ç†æ¨è
                    processed_results = index_service.result_processor.process_recommendations(
                        documents=mock_retrieved_docs,
                        query=search_query,
                        top_k=min(len(mock_retrieved_docs), 3)
                    )
                    
                    # æ ¼å¼åŒ–æ¨èç»“æœ
                    answer = format_recommendation_results(processed_results)
                    
                    # æ·»åŠ å®æ—¶æŠ“å–æ ‡è®°
                    answer = f"## å®æ—¶æœç´¢ç»“æœ\n*ä»¥ä¸‹æ˜¯é€šè¿‡æ··åˆæœç´¢è·å–çš„æœ€æ–°æ¨è*\n\n{answer}"
                else:
                    # æ²¡æœ‰è¶³å¤Ÿçš„æ–‡æ¡£ç”¨äºæ¨è
                    raise ValueError("æ²¡æœ‰è¶³å¤Ÿçš„æ–‡æ¡£ç”¨äºç”Ÿæˆæ¨è")
                    
            except Exception as e:
                logger.error(f"ä½¿ç”¨æ¨èå¤„ç†å™¨æ—¶å‡ºé”™: {str(e)}", exc_info=True)
                
                # å›é€€åˆ°ç®€å•çš„æ¨èæ ¼å¼
                answer = "# æ¨èç»“æœ (æ··åˆæœç´¢)\n\n"
                for i, post in enumerate(crawled_posts, 1):
                    title = post.thread_title if hasattr(post, 'thread_title') and post.thread_title else "æ— æ ‡é¢˜"
                    url = post.url if hasattr(post, 'url') and post.url else "#"
                    upvotes = post.upvotes if hasattr(post, 'upvotes') else 0
                    
                    # æå–æ‘˜è¦
                    content = post.content if hasattr(post, 'content') and post.content else ""
                    summary = content[:200] + "..." if len(content) > 200 else content
                    
                    answer += f"## {i}. **{title}**\n\n"
                    answer += f"- upvotes: {upvotes}\n"
                    answer += f"- [url]({url})\n\n"
                    answer += f"{summary}\n\n---\n\n"
            
            metadata = {
                'query_type': 'recommendation', 
                'processing': 'direct',
                'crawled': True,
                'real_time': True,
                'pure_real_time': False,
                'platform': platform,
                'query': search_query,
            }
            
            # å°†å¯¹è¯æ·»åŠ åˆ°è®°å¿†
            if session_id:
                MemoryService.add_to_memory(
                    session_id,
                    search_query,
                    answer
                )
            
            updated_memory = MemoryService.get_recent_memory(session_id) if session_id else []
            
            # ç›´æ¥è¿”å›ç»“æœï¼Œä¸ç»è¿‡LLMå¤„ç†
            return JsonResponse({
                'result': answer,
                'metadata': metadata,
                'llm_model': "recommendation_processor",
                'crawled': True,
                'history': updated_memory
            },
            json_dumps_params={'ensure_ascii': False}
        )

        
        # éæ¨èç±»æŸ¥è¯¢(2-6)ï¼Œä½¿ç”¨LLMå¤„ç†
        else:
            # æ„å»ºæç¤ºè¯ï¼Œç¡®ä¿ä¸generate_promptçš„æœŸæœ›æ ¼å¼ä¸€è‡´
            try:
                # åˆ›å»ºæ ‡å‡†çš„Documentå¯¹è±¡åˆ—è¡¨
                docs_for_prompt = []
                for i, post in enumerate(crawled_posts):
                    doc = Document(
                        page_content=post.content,
                        metadata={
                            "source": platform,
                            "url": post.url,
                            "title": post.thread_title,
                            "id": f"rt-{i}",  # æ·»åŠ ä¸€ä¸ªå”¯ä¸€ID
                            "upvotes": post.upvotes if hasattr(post, 'upvotes') else 0
                        }
                    )
                    docs_for_prompt.append(doc)
                
                # ä½¿ç”¨æ ‡å‡†çš„generate_promptå‡½æ•°
                prompt = generate_prompt(
                    search_query, 
                    docs_for_prompt, 
                    recent_memory, 
                    platform, 
                    classification
                )
            except Exception as e:
                logger.error(f"ç”Ÿæˆæç¤ºè¯å¼‚å¸¸: {str(e)}", exc_info=True)
                # å¤‡ç”¨ç®€æ˜“æç¤ºè¯
                combined_content = ""
                for post in crawled_posts:
                    combined_content += f"æ ‡é¢˜: {post.thread_title}\n\n"
                    combined_content += f"å†…å®¹: {post.content}\n\n"
                    combined_content += f"URL: {post.url}\n\n"
                    combined_content += "---\n\n"
                
                prompt = f"""
åŸºäºä»¥ä¸‹ä»{platform}è·å–çš„æœ€æ–°å®æ—¶ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜:

ç”¨æˆ·é—®é¢˜: {search_query}

æœç´¢ç»“æœ:
{combined_content}

è¯·æ ¹æ®ä¸Šè¿°ä¿¡æ¯æä¾›å‡†ç¡®ã€æœ‰å¸®åŠ©çš„å›ç­”ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·å¦è¯šå‘ŠçŸ¥ã€‚
"""
            
            # å‘é€è‡³AI
            future = executor.submit(
                send_prompt, 
                prompt, 
                llm_model
            )
            response = future.result()
            
            # è§£æå“åº”
            try:
                answer, metadata = parse_langchain_response(response)
            except Exception as e:
                logger.error(f"è§£æå“åº”å¼‚å¸¸: {str(e)}", exc_info=True)
                # ç®€å•å“åº”
                answer = response if isinstance(response, str) else str(response)
                metadata = {}
            
            metadata.update({
                'crawled': True, 
                'real_time': True,
                'pure_real_time': False,
                'platform': platform, 
                'posts_count': len(crawled_posts)
            })
            
            # ä¿å­˜åˆ°è®°å¿†
            if session_id:
                MemoryService.add_to_memory(session_id, search_query, answer)
            
            updated_memory = MemoryService.get_recent_memory(session_id) if session_id else []
            
            return JsonResponse({
                'result': answer,
                'metadata': metadata,
                'llm_model': llm_model,
                'crawled': True,
                'history': updated_memory
            },
            json_dumps_params={'ensure_ascii': False}
        )

        
    except Exception as e:
        logger.error(f"æ··åˆæœç´¢å¼‚å¸¸: {str(e)}", exc_info=True)
        return JsonResponse({
            'result': f"æŠ±æ­‰ï¼Œåœ¨å°è¯•è·å–å…³äº'{search_query}'çš„æœ€æ–°ä¿¡æ¯æ—¶é‡åˆ°äº†é—®é¢˜ã€‚",
            'metadata': {'error': str(e), 'crawl_attempted': True},
            'llm_model': llm_model,
            'history': recent_memory
        })

def handle_pure_real_time_crawling(search_query, platform, session_id, llm_model, recent_memory, classification=None):
    """
    å¤„ç†çº¯å®æ—¶æŠ“å–åŠŸèƒ½çš„è¾…åŠ©å‡½æ•°ã€‚
    ç›´æ¥ä»å¤–éƒ¨å¹³å°æŠ“å–æ•°æ®ï¼Œä¸è€ƒè™‘æ•°æ®åº“ä¸­æ˜¯å¦å·²æœ‰æ•°æ®ã€‚
    """
    logger.info(f"å¼€å§‹çº¯å®æ—¶æŠ“å–: {search_query}, å¹³å°: {platform}")
    
    try:
        # å¦‚æœåˆ†ç±»æœªæä¾›ï¼Œè¿›è¡Œåˆ†ç±»
        if classification is None:
            classification = re.search(r">(\d+)<", classify_query(search_query, llm_model)).group(1)
            logger.info(f"æŸ¥è¯¢'{search_query}'è¢«åˆ†ç±»ä¸º: {classification}")
        
        # æ ¹æ®å¹³å°é€‰æ‹©é€‚å½“çš„çˆ¬è™«
        if platform == 'reddit':
            try:
                from django_apps.search.reddit_crawler import create_reddit_instance, fetch_and_store_reddit_posts
                
                # åˆ›å»ºRedditå®ä¾‹
                reddit = create_reddit_instance()
                
                # ä¼˜åŒ–æŸ¥è¯¢è¯
                query_words = search_query.split()
                if len(query_words) > 3:
                    # ç§»é™¤å¸¸è§åœç”¨è¯
                    stopwords = {'a', 'an', 'the', 'is', 'are', 'was', 'were', 'what', 'which', 'how', 'why', 'when', 'who'}
                    optimized_query = ' '.join([word for word in query_words if word.lower() not in stopwords])
                else:
                    optimized_query = search_query
                    
                # å¯¹äºæ¨èç±»æŸ¥è¯¢æ·»åŠ æœç´¢ä¿®é¥°ç¬¦
                if 'recommend' in search_query.lower() or 'suggest' in search_query.lower() or classification == '1':
                    optimized_query += ' recommend OR suggest OR best'
                
                logger.info(f"ä¼˜åŒ–åçš„RedditæŸ¥è¯¢: {optimized_query}")
                
                # æŠ“å–å¸–å­
                crawled_posts = fetch_and_store_reddit_posts(reddit, optimized_query, limit=5)
                
                # åœ¨åå°å¤„ç†æ•°æ®ï¼Œè¿›è¡Œembedding
                import threading
                threading.Thread(
                    target=process_crawled_data_for_indexing,
                    args=(crawled_posts, platform),
                    daemon=True
                ).start()
                
            except Exception as e:
                logger.error(f"Redditçˆ¬è™«å¼‚å¸¸: {str(e)}", exc_info=True)
                return JsonResponse({
                    'result': f"æŠ±æ­‰ï¼Œåœ¨æŠ“å–Redditä¿¡æ¯æ—¶é‡åˆ°é—®é¢˜: {str(e)}",
                    'metadata': {'crawler_error': str(e)},
                    'llm_model': llm_model,
                    'history': recent_memory
                })
            
        elif platform == 'stackoverflow':
            try:
                from django_apps.search.stackoverflow_crawler import fetch_and_store_stackoverflow_data
                
                # å¯¹äºStackOverflowä¸éœ€è¦å¤ªå¤šä¼˜åŒ–
                crawled_posts = fetch_and_store_stackoverflow_data(search_query, limit=5)
                
                # åœ¨åå°å¤„ç†æ•°æ®ï¼Œè¿›è¡Œembedding
                import threading
                threading.Thread(
                    target=process_crawled_data_for_indexing,
                    args=(crawled_posts, platform),
                    daemon=True
                ).start()
                
            except Exception as e:
                logger.error(f"StackOverflowçˆ¬è™«å¼‚å¸¸: {str(e)}", exc_info=True)
                return JsonResponse({
                    'result': f"æŠ±æ­‰ï¼Œåœ¨æŠ“å–StackOverflowä¿¡æ¯æ—¶é‡åˆ°é—®é¢˜: {str(e)}",
                    'metadata': {'crawler_error': str(e)},
                    'llm_model': llm_model,
                    'history': recent_memory
                })
            
        elif platform == 'rednote':
            try:
                from django_apps.search.crawler import crawl_rednote_page

                target_url = generate_xhs_search_url(search_query)
                logger.info(f"Generated RedNote search URL: {target_url}")

                # ä½¿ç”¨ä» crawler_config å¯¼å…¥çš„ Cookies
                cookies_to_use = REDNOTE_LOGIN_COOKIES
                if not cookies_to_use:
                     raise ValueError("RedNote cookies not configured or empty in crawler_config.py.")

                crawled_posts = crawl_rednote_page(url=target_url, cookies=cookies_to_use, immediate_indexing=False)
                logger.info(f"RedNote crawler attempted search URL, found {len(crawled_posts)} potential posts.")
                
                # åœ¨åå°å¤„ç†æ•°æ®ï¼Œè¿›è¡Œembedding
                import threading
                threading.Thread(
                    target=process_crawled_data_for_indexing,
                    args=(crawled_posts, platform),
                    daemon=True
                ).start()

            except ImportError:
                 logger.error(f"RedNote crawler function not found.")
                 return JsonResponse({
                    'result': f"æŠ±æ­‰ï¼ŒRedNoteå®æ—¶æŠ“å–åŠŸèƒ½é…ç½®é”™è¯¯ã€‚",
                    'metadata': {'crawler_error': 'ImportError', 'crawl_attempted': True, 'platform': platform},
                    'llm_model': llm_model,
                    'history': recent_memory
                })
            except Exception as e:
                logger.error(f"RedNoteçˆ¬è™«å¼‚å¸¸ (URL: {target_url}): {str(e)}", exc_info=True) # Log URL
                return JsonResponse({
                    'result': f"æŠ±æ­‰ï¼Œåœ¨æŠ“å–RedNoteä¿¡æ¯æ—¶é‡åˆ°é—®é¢˜: {str(e)}",
                    'metadata': {'crawler_error': str(e), 'crawl_attempted': True, 'platform': platform},
                    'llm_model': llm_model,
                    'history': recent_memory
                })
        else:
            return JsonResponse({
                'result': f"æŠ±æ­‰ï¼Œå¹³å°'{platform}'ä¸æ”¯æŒå®æ—¶æŠ“å–åŠŸèƒ½ã€‚",
                'metadata': {'error': 'unsupported_platform'},
                'llm_model': llm_model,
                'history': recent_memory
            })
        
        if not crawled_posts:
            logger.warning(f"æœªæ‰¾åˆ°æŸ¥è¯¢ç»“æœ: {search_query}, å¹³å°: {platform}")
            return JsonResponse({
                'result': f"æŠ±æ­‰ï¼Œæˆ‘æ— æ³•åœ¨{platform}æ‰¾åˆ°å…³äº'{search_query}'çš„ç›¸å…³ä¿¡æ¯ã€‚",
                'metadata': {'no_results': True, 'crawl_attempted': True},
                'llm_model': llm_model,
                'history': recent_memory
            })
        
        logger.info(f"æˆåŠŸæŠ“å– {len(crawled_posts)} æ¡å†…å®¹ï¼ŒæŸ¥è¯¢: {search_query}, å¹³å°: {platform}")
        
        # å¦‚æœæ˜¯æ¨èç±»æŸ¥è¯¢(1)ï¼Œåˆ™ä½¿ç”¨ç›´æ¥å¤„ç†æ–¹å¼
        if classification == '1':
            logger.info("ä½¿ç”¨æ¨èç±»å¤„ç†é€»è¾‘å¤„ç†å®æ—¶æŠ“å–ç»“æœ")
            
            # åˆ›å»ºæ ‡å‡†æ ¼å¼çš„æ–‡æ¡£å¯¹è±¡
            mock_retrieved_docs = []
            for post in crawled_posts:
                # åˆ›å»ºç¬¦åˆDocumentæ ¼å¼çš„å¯¹è±¡
                mock_doc = Document(
                    page_content=post.content,
                    metadata={
                        "source": platform,
                        "score": post.upvotes if hasattr(post, 'upvotes') else 0,
                        "url": post.url,
                        "title": post.thread_title,
                        "thread_id": post.thread_id,
                        "author": post.author_name,
                        "upvotes": post.upvotes if hasattr(post, 'upvotes') else 0
                    }
                )
                mock_retrieved_docs.append(mock_doc)
            
            # ä½¿ç”¨ResultProcessorå¤„ç†æ¨è
            try:
                # å°è¯•ä½¿ç”¨ResultProcessorï¼Œä½†å‡†å¤‡å¥½ç›´æ¥å¤‡ç”¨æ–¹æ¡ˆ
                if len(mock_retrieved_docs) > 0:
                    # è°ƒç”¨å¤„ç†æ¨è
                    processed_results = index_service.result_processor.process_recommendations(
                        documents=mock_retrieved_docs,
                        query=search_query,
                        top_k=min(len(mock_retrieved_docs), 3)
                    )
                    
                    # æ ¼å¼åŒ–æ¨èç»“æœ
                    answer = format_recommendation_results(processed_results)
                    
                    # æ·»åŠ å®æ—¶æŠ“å–æ ‡è®°
                    answer = f"## å®æ—¶æœç´¢ç»“æœ\n*ä»¥ä¸‹æ˜¯é€šè¿‡çº¯å®æ—¶æœç´¢è·å–çš„æœ€æ–°æ¨è*\n\n{answer}"
                else:
                    # æ²¡æœ‰è¶³å¤Ÿçš„æ–‡æ¡£ç”¨äºæ¨è
                    raise ValueError("æ²¡æœ‰è¶³å¤Ÿçš„æ–‡æ¡£ç”¨äºç”Ÿæˆæ¨è")
                    
            except Exception as e:
                logger.error(f"ä½¿ç”¨æ¨èå¤„ç†å™¨æ—¶å‡ºé”™: {str(e)}", exc_info=True)
                
                # å›é€€åˆ°ç®€å•çš„æ¨èæ ¼å¼
                answer = "# æ¨èç»“æœ (çº¯å®æ—¶æŠ“å–)\n\n"
                for i, post in enumerate(crawled_posts, 1):
                    title = post.thread_title if hasattr(post, 'thread_title') and post.thread_title else "æ— æ ‡é¢˜"
                    url = post.url if hasattr(post, 'url') and post.url else "#"
                    upvotes = post.upvotes if hasattr(post, 'upvotes') else 0
                    
                    # æå–æ‘˜è¦
                    content = post.content if hasattr(post, 'content') and post.content else ""
                    summary = content[:200] + "..." if len(content) > 200 else content
                    
                    answer += f"## {i}. **{title}**\n\n"
                    answer += f"- upvotes: {upvotes}\n"
                    answer += f"- [url]({url})\n\n"
                    answer += f"{summary}\n\n---\n\n"
            
            metadata = {
                'query_type': 'recommendation', 
                'processing': 'direct',
                'crawled': True,
                'real_time': True,
                'pure_real_time': True,
                'platform': platform,
                'query': search_query,
            }
            
            # å°†å¯¹è¯æ·»åŠ åˆ°è®°å¿†
            if session_id:
                MemoryService.add_to_memory(
                    session_id,
                    search_query,
                    answer
                )
            
            updated_memory = MemoryService.get_recent_memory(session_id) if session_id else []
            
            # ç›´æ¥è¿”å›ç»“æœï¼Œä¸ç»è¿‡LLMå¤„ç†
            return JsonResponse({
                'result': answer,
                'metadata': metadata,
                'llm_model': "recommendation_processor",
                'crawled': True,
                'history': updated_memory
            },
            json_dumps_params={'ensure_ascii': False}
        )

        
        # éæ¨èç±»æŸ¥è¯¢(2-6)ï¼Œä½¿ç”¨LLMå¤„ç†
        else:
            # æ„å»ºæç¤ºè¯ï¼Œç¡®ä¿ä¸generate_promptçš„æœŸæœ›æ ¼å¼ä¸€è‡´
            try:
                # åˆ›å»ºæ ‡å‡†çš„Documentå¯¹è±¡åˆ—è¡¨
                docs_for_prompt = []
                for i, post in enumerate(crawled_posts):
                    doc = Document(
                        page_content=post.content,
                        metadata={
                            "source": platform,
                            "url": post.url,
                            "title": post.thread_title,
                            "id": f"rt-{i}",  # æ·»åŠ ä¸€ä¸ªå”¯ä¸€ID
                            "upvotes": post.upvotes if hasattr(post, 'upvotes') else 0
                        }
                    )
                    docs_for_prompt.append(doc)
                
                # ä½¿ç”¨æ ‡å‡†çš„generate_promptå‡½æ•°
                prompt = generate_prompt(
                    search_query, 
                    docs_for_prompt, 
                    recent_memory, 
                    platform, 
                    classification
                )
            except Exception as e:
                logger.error(f"ç”Ÿæˆæç¤ºè¯å¼‚å¸¸: {str(e)}", exc_info=True)
                # å¤‡ç”¨ç®€æ˜“æç¤ºè¯
                combined_content = ""
                for post in crawled_posts:
                    combined_content += f"æ ‡é¢˜: {post.thread_title}\n\n"
                    combined_content += f"å†…å®¹: {post.content}\n\n"
                    combined_content += f"URL: {post.url}\n\n"
                    combined_content += "---\n\n"
                
                prompt = f"""
åŸºäºä»¥ä¸‹ä»{platform}è·å–çš„æœ€æ–°å®æ—¶ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜:

ç”¨æˆ·é—®é¢˜: {search_query}

æœç´¢ç»“æœ:
{combined_content}

è¯·æ ¹æ®ä¸Šè¿°ä¿¡æ¯æä¾›å‡†ç¡®ã€æœ‰å¸®åŠ©çš„å›ç­”ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·å¦è¯šå‘ŠçŸ¥ã€‚
"""
            
            # å‘é€è‡³AI
            future = executor.submit(
                send_prompt, 
                prompt, 
                llm_model
            )
            response = future.result()
            
            # è§£æå“åº”
            try:
                answer, metadata = parse_langchain_response(response)
            except Exception as e:
                logger.error(f"è§£æå“åº”å¼‚å¸¸: {str(e)}", exc_info=True)
                # ç®€å•å“åº”
                answer = response if isinstance(response, str) else str(response)
                metadata = {}
            
            metadata.update({
                'crawled': True, 
                'real_time': True,
                'pure_real_time': True,
                'platform': platform, 
                'posts_count': len(crawled_posts)
            })
            
            # ä¿å­˜åˆ°è®°å¿†
            if session_id:
                MemoryService.add_to_memory(session_id, search_query, answer)
            
            updated_memory = MemoryService.get_recent_memory(session_id) if session_id else []
            
            return JsonResponse({
                'result': answer,
                'metadata': metadata,
                'llm_model': llm_model,
                'crawled': True,
                'history': updated_memory
            },
            json_dumps_params={'ensure_ascii': False}
        )

        
    except Exception as e:
        logger.error(f"çº¯å®æ—¶æŠ“å–å¼‚å¸¸: {str(e)}", exc_info=True)
        return JsonResponse({
            'result': f"æŠ±æ­‰ï¼Œåœ¨å°è¯•è·å–å…³äº'{search_query}'çš„æœ€æ–°ä¿¡æ¯æ—¶é‡åˆ°äº†é—®é¢˜ã€‚",
            'metadata': {'error': str(e), 'crawl_attempted': True},
            'llm_model': llm_model,
            'history': recent_memory
        })

@require_POST
def real_time_crawl(request):
    """
    çº¯å®æ—¶æŠ“å–ç«¯ç‚¹ï¼Œç”¨äºå‰ç«¯ç›´æ¥è°ƒç”¨å®æ—¶æŠ“å–åŠŸèƒ½ã€‚
    ç›´æ¥ä»å¤–éƒ¨å¹³å°æŠ“å–æ•°æ®ï¼Œä¸æ£€æŸ¥æ•°æ®åº“ã€‚
    """
    data = json.loads(request.body.decode('utf-8'))
    search_query = data.get('search_query')
    platform = data.get('source', 'reddit')
    session_id = data.get('session_id')
    llm_model = data.get('llm_model', 'gemini-2.0-flash')
    
    if not search_query:
        return JsonResponse({'error': 'æœªæä¾›æœç´¢æŸ¥è¯¢'}, status=400)
    
    # è·å–è®°å¿†
    recent_memory = []
    if session_id:
        recent_memory = MemoryService.get_recent_memory(session_id, limit=10, platform=platform)
    
    # è°ƒç”¨çº¯å®æ—¶æŠ“å–å¤„ç†å‡½æ•°
    return handle_pure_real_time_crawling(search_query, platform, session_id, llm_model, recent_memory)

@require_POST
def mix_search(request):
    """
    æ··åˆæœç´¢ç«¯ç‚¹ï¼Œç”¨äºå‰ç«¯è°ƒç”¨æ··åˆæœç´¢åŠŸèƒ½ã€‚
    å…ˆæ£€æŸ¥æ•°æ®åº“ï¼Œå¦‚æœæ²¡æœ‰ç»“æœå†è¿›è¡Œå®æ—¶æŠ“å–ã€‚
    """
    data = json.loads(request.body.decode('utf-8'))
    search_query = data.get('search_query')
    platform = data.get('source', 'reddit')
    session_id = data.get('session_id')
    llm_model = data.get('llm_model', 'gemini-2.0-flash')
    
    if not search_query:
        return JsonResponse({'error': 'æœªæä¾›æœç´¢æŸ¥è¯¢'}, status=400)
    
    # è·å–è®°å¿†
    recent_memory = []
    if session_id:
        recent_memory = MemoryService.get_recent_memory(session_id, limit=10, platform=platform)
    
    # è°ƒç”¨æ··åˆæœç´¢å¤„ç†å‡½æ•°
    return handle_mixed_search(search_query, platform, session_id, llm_model, recent_memory)

def generate_xhs_search_url(query: str) -> str:
    """
    ä¿®æ­£ç‰ˆå°çº¢ä¹¦æœç´¢URLç”Ÿæˆå™¨

    :param query: ç”¨æˆ·è¾“å…¥çš„æœç´¢å…³é”®è¯ï¼ˆå¦‚ï¼š"æ–°å›½ç«‹"ï¼‰
    :return: ç¬¦åˆå°çº¢ä¹¦å®é™…è§„åˆ™çš„æœç´¢URL
    """
    # ç¬¬ä¸€æ¬¡ç¼–ç ï¼ˆä»…å¤„ç†ç‰¹æ®Šå­—ç¬¦ï¼‰
    primary_encoded = quote(query, safe='', encoding='utf-8')

    # é’ˆå¯¹ä¸­æ–‡è¿›è¡ŒäºŒæ¬¡ç¼–ç ï¼ˆä»…æ›¿æ¢%ä¸º%25ï¼‰
    double_encoded = primary_encoded.replace('%', '%25')

    # ç»„åˆæœ€ç»ˆURL
    return f"https://www.xiaohongshu.com/search_result?keyword={double_encoded}&source=web_search_result_notes"

def process_crawled_data_for_indexing(crawled_posts, platform):
    """
    å¤„ç†å®æ—¶æŠ“å–åˆ°çš„æ•°æ®ï¼Œå°†å…¶ä¿å­˜åˆ°æ•°æ®åº“å¹¶æŒ‰ç…§ç°æœ‰æµç¨‹è¿›è¡Œembeddingã€‚
    å¢åŠ å¯¹thread_idçš„æ£€æŸ¥ï¼Œå¦‚æœå‘ç°é‡å¤åˆ™åˆ é™¤æ•°æ®åº“ä¸­çš„è®°å½•ã€‚
    
    Args:
        crawled_posts: æŠ“å–åˆ°çš„æ•°æ®åˆ—è¡¨
        platform: å¹³å°åç§° ('reddit', 'stackoverflow', 'rednote')
        
    Returns:
        å¤„ç†åçš„ç»“æœä¿¡æ¯
    """
    try:
        if not crawled_posts:
            logger.info("æ²¡æœ‰æ•°æ®éœ€è¦å¤„ç†")
            return {"success": False, "message": "æ²¡æœ‰æ•°æ®éœ€è¦å¤„ç†", "processed_count": 0}
        
        logger.info(f"å¼€å§‹å¤„ç†{len(crawled_posts)}æ¡æŠ“å–æ•°æ®ï¼Œå¹³å°: {platform}")
        
        # è·å–å¹³å°å¯¹åº”çš„æ¨¡å‹ç±»
        model_class_map = {
            'reddit': RedditContent,
            'stackoverflow': StackOverflowContent,
            'rednote': RednoteContent
        }
        
        if platform not in model_class_map:
            logger.error(f"æœªçŸ¥å¹³å°: {platform}")
            return {"success": False, "message": f"æœªçŸ¥å¹³å°: {platform}", "processed_count": 0}
            
        model_class = model_class_map[platform]
        
        # åˆ›å»ºæˆ–è·å–IndexServiceå®ä¾‹
        global index_service
        # ç¡®ä¿ç´¢å¼•æœåŠ¡ä½¿ç”¨æ­£ç¡®çš„å¹³å°
        if index_service.platform != platform:
            index_service.platform = platform
            index_service.faiss_manager.set_platform(platform)
        
        # ç¡®ä¿å·²åŠ è½½ç´¢å¼•
        index_service.faiss_manager.load_index()
        
        # æ”¶é›†æ‰€æœ‰æŠ“å–çš„thread_idsï¼Œç”¨äºä¸€æ¬¡æ€§æŸ¥è¯¢å·²å­˜åœ¨çš„è®°å½•
        thread_ids = [post.thread_id for post in crawled_posts if hasattr(post, 'thread_id') and post.thread_id]
        
        # æŸ¥è¯¢æ•°æ®åº“ä¸­å·²å­˜åœ¨ä¸”å·²embeddingçš„è®°å½•
        existing_records = list(model_class.objects.filter(
            thread_id__in=thread_ids, 
            embedding_key__isnull=False  # å·²ç»æœ‰embedding_keyçš„è®°å½•
        ).values_list('thread_id', flat=True))
        
        # è®°å½•å“ªäº›postçš„IDæ˜¯åˆšåˆšæ·»åŠ çš„ä½†æ˜¯ä¸å·²æœ‰è®°å½•é‡å¤
        duplicate_ids = []
        
        for post in crawled_posts:
            if not hasattr(post, 'thread_id') or not post.thread_id:
                continue
                
            if post.thread_id in existing_records:
                logger.info(f"å‘ç°é‡å¤è®°å½•ï¼Œthread_id: {post.thread_id}")
                duplicate_ids.append(post.id)
                
        # ä¸€æ¬¡æ€§åˆ é™¤æ‰€æœ‰é‡å¤è®°å½•
        if duplicate_ids:
            deletion_result = model_class.objects.filter(id__in=duplicate_ids).delete()
            logger.info(f"å·²åˆ é™¤ {deletion_result[0]} æ¡é‡å¤è®°å½•")
        
        # é‡æ–°è¿‡æ»¤crawled_postsï¼Œåªä¿ç•™æœªåˆ é™¤çš„è®°å½•
        filtered_posts = [post for post in crawled_posts if post.id not in duplicate_ids]
        
        # é€ä¸ªå¤„ç†æŠ“å–åˆ°çš„æ¡ç›®
        processed_count = 0
        skipped_count = 0
        
        for post in filtered_posts:
            if not hasattr(post, 'content') or not post.content:
                logger.warning(f"è·³è¿‡æ²¡æœ‰å†…å®¹çš„æ¡ç›®: {post.id}")
                skipped_count += 1
                continue
                
            # ç°åœ¨å¤„ç†ç¡®è®¤ä¸å­˜åœ¨ä¸”æœ‰å†…å®¹çš„æ¡ç›®
            try:
                # ä½¿ç”¨é¡¹ç›®ä¸­ç°æœ‰çš„index_crawled_itemæ–¹æ³•è¿›è¡Œå¤„ç†
                # è¿™å°†ç”Ÿæˆembeddingï¼Œæ·»åŠ åˆ°FAISSï¼Œå¹¶è®¾ç½®embedding_key
                index_service.indexer.index_crawled_item(post, post.content, save_index=False)
                processed_count += 1
            except Exception as e:
                logger.error(f"å¤„ç†æ¡ç›®æ—¶å‡ºé”™ (ID: {post.id}): {str(e)}", exc_info=True)
        
        # æ‰¹é‡ä¿å­˜ç´¢å¼•
        if processed_count > 0:
            index_service.faiss_manager.save_index()
            logger.info(f"å®Œæˆå¤„ç† {processed_count}/{len(filtered_posts)} æ¡æ•°æ®ï¼Œåˆ é™¤ {len(duplicate_ids)} æ¡é‡å¤æ•°æ®ï¼Œè·³è¿‡ {skipped_count} æ¡æ— å†…å®¹æ•°æ®ï¼Œå¹³å°: {platform}")
        
        return {
            "success": True,
            "message": f"æˆåŠŸå¤„ç† {processed_count} æ¡æ•°æ®ï¼Œåˆ é™¤ {len(duplicate_ids)} æ¡é‡å¤æ•°æ®",
            "processed_count": processed_count,
            "deleted_count": len(duplicate_ids)
        }
        
    except Exception as e:
        logger.error(f"å¤„ç†æŠ“å–æ•°æ®æ—¶å‡ºé”™: {str(e)}", exc_info=True)
        return {"success": False, "message": f"å¤„ç†å¤±è´¥: {str(e)}", "processed_count": 0}

